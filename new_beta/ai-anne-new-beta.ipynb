{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd813a5-c111-461f-821b-38bce5c43430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 12.9150\n",
      "Epoch 11, Loss: 3.9324\n",
      "Epoch 21, Loss: 1.6254\n",
      "Epoch 31, Loss: 1.0154\n",
      "Epoch 41, Loss: 0.7056\n",
      "Epoch 51, Loss: 0.5174\n",
      "Epoch 61, Loss: 0.3950\n",
      "Epoch 71, Loss: 0.3116\n",
      "Epoch 81, Loss: 0.2530\n",
      "Epoch 91, Loss: 0.2105\n",
      "Epoch 100, Loss: 0.1813\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# Seed for reproducibility\n",
    "random.seed(0)\n",
    "\n",
    "# Activation functions and derivatives\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(output):\n",
    "    return output * (1 - output)\n",
    "\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "def relu_derivative(output):\n",
    "    return 1 if output > 0 else 0\n",
    "\n",
    "# Initialize weights and biases\n",
    "def init_layer(input_size, output_size):\n",
    "    weights = [[random.uniform(-0.5, 0.5) for _ in range(input_size)] for _ in range(output_size)]\n",
    "    biases = [random.uniform(-0.5, 0.5) for _ in range(output_size)]\n",
    "    return weights, biases\n",
    "\n",
    "# Forward pass\n",
    "def dense_forward(inputs, weights, biases, activation='relu'):\n",
    "    outputs = []\n",
    "    pre_activations = []\n",
    "    for w, b in zip(weights, biases):\n",
    "        z = sum(i*w_ij for i, w_ij in zip(inputs, w)) + b\n",
    "        pre_activations.append(z)\n",
    "        if activation == 'sigmoid':\n",
    "            outputs.append(sigmoid(z))\n",
    "        elif activation == 'relu':\n",
    "            outputs.append(relu(z))\n",
    "        else:\n",
    "            raise Exception(\"Unknown activation\")\n",
    "    return outputs, pre_activations\n",
    "\n",
    "# Backward pass\n",
    "def dense_backward(inputs, grad_outputs, outputs, pre_activations, weights, biases, activation='relu', lr=0.01):\n",
    "    input_grads = [0.0 for _ in range(len(inputs))]\n",
    "    for j in range(len(weights)):\n",
    "        if activation == 'sigmoid':\n",
    "            delta = grad_outputs[j] * sigmoid_derivative(outputs[j])\n",
    "        elif activation == 'relu':\n",
    "            delta = grad_outputs[j] * relu_derivative(pre_activations[j])\n",
    "        else:\n",
    "            raise Exception(\"Unknown activation\")\n",
    "        for i in range(len(inputs)):\n",
    "            input_grads[i] += weights[j][i] * delta\n",
    "            weights[j][i] -= lr * delta * inputs[i]\n",
    "        biases[j] -= lr * delta\n",
    "    return input_grads\n",
    "\n",
    "# Loss function\n",
    "def binary_cross_entropy(predicted, target):\n",
    "    epsilon = 1e-7\n",
    "    return - (target * math.log(predicted + epsilon) + (1 - target) * math.log(1 - predicted + epsilon))\n",
    "\n",
    "def binary_cross_entropy_derivative(predicted, target):\n",
    "    epsilon = 1e-7\n",
    "    return -(target / (predicted + epsilon)) + (1 - target) / (1 - predicted + epsilon)\n",
    "\n",
    "# Dataset\n",
    "X = [[ 0.81575475, -0.21746808, -0.12904165, -0.65303909],\n",
    "         [ 0.05761837,  1.59476592,  0.84485761,  1.71304456],\n",
    "         [ 0.96738203,  0.68864892, -0.00730424, -0.41643072],\n",
    "         [ 2.02877297,  0.38660992,  2.06223168,  1.00321947],\n",
    "         [ 1.42226386,  0.99068792,  1.33180724,  0.29339437],\n",
    "         [ 0.81575475,  0.99068792,  1.21006983,  1.4764362 ],\n",
    "         [-1.00377258,  0.38660992, -0.49425387, -0.41643072],\n",
    "         [ 0.05761837, -0.51950708, -0.00730424,  0.29339437],\n",
    "         [ 0.36087292,  0.38660992,  1.08833242,  1.23982783],\n",
    "         [ 0.66412748,  0.38660992,  0.35790798,  1.4764362 ],\n",
    "         [ 0.05761837,  0.08457092,  0.84485761,  0.29339437],\n",
    "         [-0.70051802, -0.51950708,  0.23617057,  0.53000274],\n",
    "         [ 0.20924564, -0.21746808,  0.84485761,  1.00321947],\n",
    "         [-0.24563619,  0.08457092, -0.25077906, -0.65303909],\n",
    "         [-2.06516352, -1.42562408, -1.95510276, -1.59947255],\n",
    "         [-1.15539985, -1.42562408, -1.34641572, -1.36286418],\n",
    "         [ 0.05761837, -1.12358508, -0.00730424, -0.41643072],\n",
    "         [ 0.20924564,  0.08457092, -0.73772869, -0.88964745],\n",
    "         [-0.39726347, -0.51950708,  0.23617057, -0.17982236],\n",
    "         [ 0.5125002 ,  0.08457092, -0.37251647, -0.88964745]]\n",
    "\n",
    "y = [0,1,0,1,1,1,0,1,1,1,1,1,1,0,0,0,0,0,0,0]\n",
    "\n",
    "# Initialize network\n",
    "w1, b1 = init_layer(4, 3)\n",
    "w2, b2 = init_layer(3, 1)\n",
    "\n",
    "# Training\n",
    "epochs = 100\n",
    "lr = 0.05\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for xi, yi in zip(X, y):\n",
    "        # Forward pass\n",
    "        out1, pre1 = dense_forward(xi, w1, b1, 'relu')\n",
    "        out2, pre2 = dense_forward(out1, w2, b2, 'sigmoid')\n",
    "        loss = binary_cross_entropy(out2[0], yi)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Backward pass\n",
    "        dL_dout2 = [binary_cross_entropy_derivative(out2[0], yi)]\n",
    "        dL_dout1 = dense_backward(out1, dL_dout2, out2, pre2, w2, b2, 'sigmoid', lr)\n",
    "        _ = dense_backward(xi, dL_dout1, out1, pre1, w1, b1, 'relu', lr)\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94781b5f-4de4-435d-bb45-9ea8be2d9c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000\n",
      "Confusion Matrix:\n",
      "TN: 10, FP: 0\n",
      "FN: 0, TP: 10\n"
     ]
    }
   ],
   "source": [
    "def predict(x):\n",
    "    out1, _ = dense_forward(x, w1, b1, 'relu')\n",
    "    out2, _ = dense_forward(out1, w2, b2, 'sigmoid')\n",
    "    return 1 if out2[0] > 0.5 else 0\n",
    "\n",
    "ypred = [predict(xi) for xi in X]\n",
    "\n",
    "def classification_report(ytrue, ypred):\n",
    "    TP = TN = FP = FN = 0\n",
    "    for true, pred in zip(ytrue, ypred):\n",
    "        if true == pred:\n",
    "            if true == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "        else:\n",
    "            if true == 1:\n",
    "                FN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    accuracy = (TP + TN) / len(ytrue)\n",
    "    print(\"Accuracy: {:.3f}\".format(accuracy))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"TN: {}, FP: {}\".format(TN, FP))\n",
    "    print(\"FN: {}, TP: {}\".format(FN, TP))\n",
    "\n",
    "# Generate predictions\n",
    "ypred = [predict(xi) for xi in X]\n",
    "\n",
    "# Show classification metrics\n",
    "classification_report(y, ypred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed9428-2b80-4973-99a0-27a418c323e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
