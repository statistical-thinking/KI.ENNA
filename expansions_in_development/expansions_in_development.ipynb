{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc2b15b0-cfbb-4413-a54e-ea07f37328a9",
   "metadata": {},
   "source": [
    "# Explainable Artificial Intelligence with MicroPython\n",
    "#### A comprehensive codebook by Prof. Dr. habil. Dennis Klinkhammer (2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be470f7-97f5-410b-8d3f-216aadf85cf8",
   "metadata": {},
   "source": [
    "## Statistical Basics - I\n",
    "### Mean, Variance and Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff51e3-2da4-4bcc-bdaa-a6f7902f24ba",
   "metadata": {},
   "source": [
    "One variable of the **trees dataset**, provided by Atkinson, A. C. (1985): *Plots, Transformations and Regression* via Oxford University Press:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9b29baa-231c-4524-87e1-81bf021c5ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girth (x) of Black Cherry Trees\n",
    "x = [8.3, 8.6, 8.8, 10.5, 10.7, 10.8, 11, 11, 11.1, 11.2, 11.3, \n",
    "     11.4, 11.4, 11.7, 12, 12.9, 12.9, 13.3, 13.7, 13.8, 14, \n",
    "     14.2, 14.5, 16, 16.3, 17.3, 17.5, 17.9, 18, 18, 20.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac4456c-dff9-4870-900c-5b80bd1e2a31",
   "metadata": {},
   "source": [
    "The **mean**, also known as the arithmetic mean, is one of the most common measures of central tendency in statistics. It represents the average value of a dataset and provides a single value that summarizes the entire data distribution. To calculate the mean, you sum all values in your dataset and divide this total by the number of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dfa1ddf-153d-4e35-8771-84cba0094232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "def mean(data):\n",
    "    return sum(data) / len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a79a9f-1a83-4e53-b343-7c703d0a2a8a",
   "metadata": {},
   "source": [
    "The **sample variance** is a measure of how spread out the values in a dataset are. It quantifies the average squared deviation from the mean, giving insight into the variability within the sample. Unlike population variance, it divides by n−1n−1 to account for the degrees of freedom, making it an unbiased estimator when working with a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc871ff2-d1ca-4f37-af29-cf17162483e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance\n",
    "def variance(data):\n",
    "    m = mean(data)\n",
    "    return sum((x - m) ** 2 for x in data) / (len(data) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43581b1-8b0d-4af6-af57-76e635fad585",
   "metadata": {},
   "source": [
    "The **standard deviation** is the square root of the variance and provides a measure of spread in the same units as the original data. It indicates how much the values in a dataset typically deviate from the mean, making it easier to interpret than variance in practical terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c6840eb-173c-476a-a44a-3399da095331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Deviation\n",
    "def std_dev(data):\n",
    "    return variance(data) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf403c-e788-4fab-9402-a7f57d0c52c1",
   "metadata": {},
   "source": [
    "These are **application examples** for mean, sample variance and standard deviation in MicroPython:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8751d41-526d-422d-b8b6-0e661d195bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 13.248387096774193\n",
      "Variance 9.847913978494624\n",
      "Standard Deviation 3.1381386168387504\n"
     ]
    }
   ],
   "source": [
    "# Application Examples\n",
    "print(\"Mean\", mean(x))\n",
    "print(\"Variance\", variance(x))\n",
    "print(\"Standard Deviation\", std_dev(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e60abe2-36a6-4701-a2a0-0949bb6f4fe2",
   "metadata": {},
   "source": [
    "## Statistical Basics - II\n",
    "### Covariance, Correlation and Single Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ae0cf-1dd9-4f0c-a116-aef473462e82",
   "metadata": {},
   "source": [
    "Two variables of the **trees dataset**, provided by Atkinson, A. C. (1985): *Plots, Transformations and Regression* via Oxford University Press:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c060aa37-2c26-41ad-ba66-f32571bf99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girth (x) and Volume (y) of Black Cherry Trees\n",
    "x = [8.3, 8.6, 8.8, 10.5, 10.7, 10.8, 11, 11, 11.1, 11.2, 11.3, \n",
    "     11.4, 11.4, 11.7, 12, 12.9, 12.9, 13.3, 13.7, 13.8, 14, \n",
    "     14.2, 14.5, 16, 16.3, 17.3, 17.5, 17.9, 18, 18, 20.6]\n",
    "\n",
    "y = [10.3, 10.3, 10.2, 16.4, 18.8, 19.7, 15.6, 18.2, 22.6, 19.9,\n",
    "     24.2, 21, 21.4, 21.3, 19.1, 22.2, 33.8, 27.4, 25.7, 24.9,\n",
    "     34.5, 31.7, 36.3, 38.3, 42.6, 55.4, 55.7, 58.3, 51.5, 51, 77]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6c3e0-0b50-4f89-b7d5-40a1ae73a2d5",
   "metadata": {},
   "source": [
    "The **covariance** measures the directional relationship between two variables. A positive covariance indicates that the variables tend to increase together, while a negative covariance suggests that as one increases, the other tends to decrease. It's a foundational concept in statistics for understanding how two variables vary together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b28c002c-fdcf-4f53-b93c-c1b029e1f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance\n",
    "def covariance(x, y):\n",
    "    mx = mean(x)\n",
    "    my = mean(y)\n",
    "    return sum((x[i] - mx) * (y[i] - my) for i in range(len(x))) / (len(x) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25314fc-07eb-4d07-b91f-082a98d6f6c5",
   "metadata": {},
   "source": [
    "The **correlation** quantifies the strength and direction of the linear relationship between two variables. It standardizes the covariance by dividing it by the product of the standard deviations, resulting in a value between -1 and 1. A correlation close to 1 or -1 indicates a strong relationship, while a value near 0 suggests little to no linear association:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed232c0a-8090-41e9-9535-474fdb9460de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation\n",
    "def correlation(x, y):\n",
    "    return covariance(x, y) / (std_dev(x) * std_dev(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b311e-8071-4cd7-9911-0b5270333c03",
   "metadata": {},
   "source": [
    "A **simple linear regression** models the relationship between two variables by fitting a straight line to the data. It calculates the slope b and intercept a of the line y=a+bx, where b indicates how much y changes for each unit increase in x, and a is the predicted value of y when x=0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30a5cb22-a7e8-4c5f-9c35-072c089f9bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression\n",
    "def linear_regression(x, y):\n",
    "    b = covariance(x, y) / variance(x)\n",
    "    a = mean(y) - b * mean(x)\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1869c6f-9bb5-42cd-99ee-e8dc58793030",
   "metadata": {},
   "source": [
    "The **predict function** is required to determine the respective y values for the underlying x values via a and b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d5932f8-4db3-49e6-87d8-9f21391c09aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Function\n",
    "def predict(x_new, a, b):\n",
    "    return a + b * x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8feef56-f685-41e5-acb4-6d4541cc0af6",
   "metadata": {},
   "source": [
    "**Residuals** represent the differences between the observed values and the predicted values from a linear regression model. They indicate how well the model fits the data: a residual close to 0 means a good fit, while larger residuals suggest that the model doesn't capture the data as accurately. The residuals can be used to assess the assumptions of linear regression and identify any outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd05fb58-7484-47bd-aff5-4edff4e75761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals\n",
    "def residuals(x, y, a, b):\n",
    "    return [y[i] - (a + b * x[i]) for i in range(len(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832348e8-a7ce-4c05-b99b-e4164dfda8ec",
   "metadata": {},
   "source": [
    "The **coefficient of determination** measures the proportion of variance in the dependent variable that is explained by the independent variable in a regression model. It indicates the goodness of fit: an coefficient of determination close to 1 means that the model explains most of the variance, while a value near 0 suggests the model doesn’t capture much of the variability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0ac66da-0f53-4b7b-9c71-cdaccd777386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient of Determination\n",
    "def r_squared(x, y, a, b):\n",
    "    y_mean = mean(y)\n",
    "    ss_tot = sum((yi - y_mean) ** 2 for yi in y)\n",
    "    ss_res = sum((y[i] - (a + b * x[i])) ** 2 for i in range(len(y)))\n",
    "    return 1 - ss_res / ss_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acdbc67-fbec-4df0-8849-6ee1d595c772",
   "metadata": {},
   "source": [
    "These are **application examples** for covariance, correlation, as well as the single linear regression with the coresponding predictions, residuals and the coefficient of determination in MicroPython:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4006c98-62e9-4ba2-a6b4-b7954d04c846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance: 49.888118279569895\n",
      "Correlation: 0.9671193682556305\n",
      "\n",
      "Single Linear Regression: y = -36.94 + 5.07 * x\n",
      "Predictions for x = 11.4: 20.8073040958404\n",
      "\n",
      "Residuals: [5.196850814975267, 3.6770938881221404, 2.563922603553383, 0.15196668471898533, 1.5387954001502386, 1.9322097578658521, -3.1809615267028963, -0.5809615267028967, 3.3124528310127275, 0.10586718872835377, 3.8992815464439694, 0.19269590415959925, 0.5926959041595978, -1.0270610226935268, -4.74681794954666, -6.20608873010605, 5.393911269893948, -3.0324312992435623, -6.758773868381059, -8.065359510665438, 0.5214692047658076, -3.291702079802949, -0.21145900665607087, -5.810243640921726, -3.030000567774856, 4.704143009381376, 3.990971724812624, 4.564629155675121, -2.7419564866092543, -3.2419564866092543, 9.586816813996947]\n",
      "\n",
      "Coefficient of Determination: 0.93531987245517\n"
     ]
    }
   ],
   "source": [
    "# Apllication Examples\n",
    "print(\"Covariance:\", covariance(x, y))\n",
    "print(\"Correlation:\", correlation(x, y))\n",
    "\n",
    "a, b = linear_regression(x, y)\n",
    "print(\"\\nSingle Linear Regression: y = {:.2f} + {:.2f} * x\".format(a, b))\n",
    "print(\"Predictions for x = 11.4:\", predict(11.4, a, b))\n",
    "print(\"\\nResiduals:\", residuals(x, y, a, b))\n",
    "print(\"\\nCoefficient of Determination:\", r_squared(x, y, a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec27fa0-b62e-4b45-89a9-4e60d41bd58d",
   "metadata": {},
   "source": [
    "## Machine Learning: Regression\n",
    "### Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bddd3c7-1fa2-44e6-83e7-37af516de606",
   "metadata": {},
   "source": [
    "Three variables of the **trees dataset**, provided by Atkinson, A. C. (1985): *Plots, Transformations and Regression* via Oxford University Press:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc5b1a9d-be90-4639-9fe2-1c8fb20915a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girth (x1), Height (x2) and Volume (y) of Black Cherry Trees \n",
    "X = [[8.3, 70], [8.6, 65], [8.8, 63], [10.5, 72], [10.7, 81], [10.8, 83], [11, 66], [11, 75], [11.1, 80], [11.2, 75],\n",
    "    [11.3, 79], [11.4, 76], [11.4, 76], [11.7, 69], [12, 75], [12.9, 74], [12.9, 85], [13.3, 86], [13.7, 71], [13.8, 64],\n",
    "    [14, 78], [14.2, 80], [14.5, 74], [16, 72], [16.3, 77], [17.3, 81], [17.5, 82], [17.9, 80], [18, 80], [18, 80], [20.6, 87]]\n",
    "\n",
    "y = [10.3, 10.3, 10.2, 16.4, 18.8, 19.7, 15.6, 18.2, 22.6, 19.9,\n",
    "     24.2, 21, 21.4, 21.3, 19.1, 22.2, 33.8, 27.4, 25.7, 24.9,\n",
    "     34.5, 31.7, 36.3, 38.3, 42.6, 55.4, 55.7, 58.3, 51.5, 51, 77]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ea49aa-3b27-401f-9e16-6e429235a431",
   "metadata": {},
   "source": [
    "**Matrix inversion** is essential in solving systems of linear equations, particularly in methods like multiple linear regression. The following code implements the Gaussian elimination method to invert a matrix, ensuring it is invertible by checking for non-zero pivots during the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddee2fb4-0a5d-4133-bcef-a461c8bcface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Matrix Inversion\n",
    "def invert_matrix(matrix):\n",
    "    n = len(matrix)\n",
    "    identity = [[float(i == j) for j in range(n)] for i in range(n)]\n",
    "    m = [row[:] for row in matrix]\n",
    "\n",
    "    for i in range(n):\n",
    "        max_row = i\n",
    "        max_val = abs(m[i][i])\n",
    "        for k in range(i + 1, n):\n",
    "            if abs(m[k][i]) > max_val:\n",
    "                max_val = abs(m[k][i])\n",
    "                max_row = k\n",
    "\n",
    "        if max_val == 0:\n",
    "            raise ValueError(\"Matrix is not invertible!\")\n",
    "\n",
    "        if max_row != i:\n",
    "            m[i], m[max_row] = m[max_row], m[i]\n",
    "            identity[i], identity[max_row] = identity[max_row], identity[i]\n",
    "\n",
    "        factor = m[i][i]\n",
    "        for j in range(n):\n",
    "            m[i][j] /= factor\n",
    "            identity[i][j] /= factor\n",
    "\n",
    "        for k in range(n):\n",
    "            if k != i:\n",
    "                factor = m[k][i]\n",
    "                for j in range(n):\n",
    "                    m[k][j] -= factor * m[i][j]\n",
    "                    identity[k][j] -= factor * identity[i][j]\n",
    "\n",
    "    return identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b855e19-bdb2-49d5-a101-c3e11386d9e6",
   "metadata": {},
   "source": [
    "**Matrix transposition** involves flipping a matrix over its diagonal, converting rows into columns and vice versa. The resulting matrix is called the transpose of the original matrix. Transposition is commonly used in linear algebra, especially in operations like solving systems of equations or adjusting data representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6a0959f-336b-46e7-a90c-d8696bf7d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Matrix Transposition\n",
    "def transpose(matrix):\n",
    "    return [[row[i] for row in matrix] for i in range(len(matrix[0]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88306231-06b6-439c-85c2-1b4f7c0772ef",
   "metadata": {},
   "source": [
    "**Matrix multiplication** is a way of combining two matrices to create a new one. This operation is essential in many areas of linear algebra, including solving systems of linear equations and applying transformations. It is important for multiple linear regression because it allows you to calculate the coefficients of the regression model by multiplying the inverse of the design matrix with the target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ec4fcff-2109-422b-a0eb-5ed655f23716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Matrix Multiplication\n",
    "def matmul(A, B):\n",
    "    result = []\n",
    "    for i in range(len(A)):\n",
    "        row = []\n",
    "        for j in range(len(B[0])):\n",
    "            val = sum(A[i][k] * B[k][j] for k in range(len(B)))\n",
    "            row.append(val)\n",
    "        result.append(row)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac6552-3ce6-4ec8-8226-3ecacc1ca65e",
   "metadata": {},
   "source": [
    "With these mathematical basics, the **multiple linear regression** can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95f97c72-588c-4468-b155-b2c4882b98d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    " # Multiple Linear Regression\n",
    "def multivariate_regression(X_raw, y):\n",
    "    X = [[1] + row for row in X_raw]\n",
    "    y_vec = [[val] for val in y]\n",
    "    \n",
    "    XT = transpose(X)\n",
    "    XTX = matmul(XT, X)\n",
    "    XTX_inv = invert_matrix(XTX)\n",
    "    XTy = matmul(XT, y_vec)\n",
    "    \n",
    "    beta = matmul(XTX_inv, XTy)\n",
    "    return [b[0] for b in beta]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a957264e-079d-4bd3-8e81-7c236c52b290",
   "metadata": {},
   "source": [
    "A slightly modified **predict function** is required to determine the respective y values for the underlying x values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1893ed4-24b0-4dda-9350-74997464512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Function\n",
    "def predict_multi(X_raw, beta):\n",
    "    X = [[1] + row for row in X_raw]\n",
    "    return [sum(b * x for b, x in zip(beta, row)) for row in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32746a-8c22-4f9c-bc17-a860a25d7ae1",
   "metadata": {},
   "source": [
    "Again, **residuals** represent the differences between the observed values and the predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0435ccc-7193-4e08-bc93-4e7f6e6b05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals\n",
    "def residuals_multi(X_raw, y, beta):\n",
    "    y_pred = predict_multi(X_raw, beta)\n",
    "    return [yi - y_hat for yi, y_hat in zip(y, y_pred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce7130-c5c8-4b85-8bc0-6a9b70171653",
   "metadata": {},
   "source": [
    "The **coefficient of determination** for a multiple linear regression model measures how well the model's predictions match the actual data. It indicates the proportion of the variance in the target variable that can be explained by the model.  It's interpretation is therefore similar to the coefficient of determination of a single linear regression model and may vary between 0 and 1, while a value closer to 0 means the model doesn't explain much of the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee1f839e-cf8a-4d4e-ab14-fdea0e1d6424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient of Determination\n",
    "def r_squared_multi(X_raw, y, beta):\n",
    "    y_pred = predict_multi(X_raw, beta)\n",
    "    y_mean = sum(y) / len(y)\n",
    "    \n",
    "    ss_tot = sum((yi - y_mean) ** 2 for yi in y)\n",
    "    ss_res = sum((yi - y_hat) ** 2 for yi, y_hat in zip(y, y_pred))\n",
    "    \n",
    "    return 1 - ss_res / ss_tot if ss_tot != 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f4eb2b-b2b2-4051-ab5a-838a44ddfbc1",
   "metadata": {},
   "source": [
    "Finally, these are **application examples** for the multiple linear regression coefficients with coresponding predictions for one case, the residuals of the model as well as the coefficient of determination in MicroPython:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e9f5866-1df6-4520-857d-02772e98468f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      "a = -57.987658918381555\n",
      "b1 = 4.708160503017467\n",
      "b2 = 0.3392512342447134\n",
      "\n",
      "Predictions for x1 = 11.4 and x2 = 76: 21.46846461861579\n",
      "\n",
      "Residuals: [5.462340346206641, 5.746148366524974, 5.3830187344109, 0.5258847710787862, -1.069008437727124, -1.3183269565183018, -0.5926880749616661, -1.0459491831640868, 1.186978595310606, -0.2875812837675795, 2.184597728951818, -0.4684646186157906, -0.068464618615792, 0.7938458701919693, -4.854109686181552, -5.652202904652565, 2.2160335186555855, -6.4064819167961105, -4.900977604332386, -3.797035014921157, 0.11181560504937238, -4.30831896404354, 0.9147402905194895, -3.4689979955172845, -2.2777023176460958, 4.457132242357581, 3.4762489075093868, 4.871487174791824, -2.399328875509923, -2.899328875509923, 8.484695176931666]\n",
      "\n",
      "Coefficient of Determination: 0.9479500377816745\n"
     ]
    }
   ],
   "source": [
    "# Application Example\n",
    "beta = multivariate_regression(X, y)\n",
    "print(\"Coefficients:\")\n",
    "for i, b in enumerate(beta):\n",
    "    if i == 0:\n",
    "        print(\"a =\", b)\n",
    "    else:\n",
    "        print(\"b{} = {}\".format(i, b))\n",
    "\n",
    "x_case_13 = [X[12]]\n",
    "y_pred_13 = predict_multi(x_case_13, beta)[0]\n",
    "print(\"\\nPredictions for x1 = 11.4 and x2 = 76:\", y_pred_13)\n",
    "\n",
    "residuals = residuals_multi(X, y, beta)\n",
    "print(\"\\nResiduals:\", residuals)\n",
    "\n",
    "r2 = r_squared_multi(X, y, beta)\n",
    "print(\"\\nCoefficient of Determination:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5c822-e74d-4203-9702-c5c3ad840d2c",
   "metadata": {},
   "source": [
    "## Machine Learning: Classification\n",
    "### Multiple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45842089-f9ce-4c8c-8fdd-9277b4fbad40",
   "metadata": {},
   "source": [
    "Three variables of the **trees dataset**, provided by Atkinson, A. C. (1985): *Plots, Transformations and Regression* via Oxford University Press. The dependant variable has been dichotomized, whereby a volume greater than 20 results in 1, else 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a2be138-cc96-4ed2-a667-fea18753c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girth (x1), Height (x2) and Binary Volume (y) of Black Cherry Trees\n",
    "X = [[8.3, 70], [8.6, 65], [8.8, 63], [10.5, 72], [10.7, 81], [10.8, 83], [11, 66], [11, 75], [11.1, 80], [11.2, 75],\n",
    "    [11.3, 79], [11.4, 76], [11.4, 76], [11.7, 69], [12, 75], [12.9, 74], [12.9, 85], [13.3, 86], [13.7, 71], [13.8, 64],\n",
    "    [14, 78], [14.2, 80], [14.5, 74], [16, 72], [16.3, 77], [17.3, 81], [17.5, 82], [17.9, 80], [18, 80], [18, 80], [20.6, 87]]\n",
    "\n",
    "\n",
    "y = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bcf888-fdd8-49ba-a48c-ac9e5c5ea9e1",
   "metadata": {},
   "source": [
    "The **sigmoid function** in (multiple) logistic regression maps any input value to a range between 0 and 1, allowing us to interpret the result as a probability by producing an S-shaped curve ideal for binary classification with 0=no and 1=yes, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31090e0b-f743-431c-901e-a94a43c353c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Sigmoid Function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + pow(2.71828, -z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9f9d4-d196-4158-ab04-d962feea5ec8",
   "metadata": {},
   "source": [
    "The **log function** approximates the natural logarithm using a numerical method based on the limit definition, useful when built-in log functions are unavailable in MicroPython. The natural logarithm (ln) is the inverse of the exponential function and tells us how many times we must multiply e≈2.71828 to get a given number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c40867a5-b5fb-4806-821b-ab14ba743f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Log Function\n",
    "def log(x):\n",
    "    n = 1000.0\n",
    "    return n * ((x**(1/n)) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232ca42f-8e76-4c8a-9ecd-4b8a8015947d",
   "metadata": {},
   "source": [
    "As a result of these mathematical basics, a function for the **prediction of probabilities** is required for processing the values of the previous sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1e5504c-4bff-4a0e-b934-28f7c65a1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Prediction of Probability\n",
    "def predict_proba(x_input, weights, bias):\n",
    "    z = bias\n",
    "    for i in range(len(x_input)):\n",
    "        z += weights[i] * x_input[i]\n",
    "    return sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbeab7e-cd55-4337-9ade-89d56ea02a7b",
   "metadata": {},
   "source": [
    "This function trains a logistic regression model using **gradient descent**. It iteratively updates the weights and bias to minimize the error between predicted probabilities (from the sigmoid function) and actual labels. By adjusting the weights in the direction that reduces the loss, the model gradually learns to classify input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fba09a35-fb83-4789-adf9-b81642424a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate Logistic Regression via Gradient Descent\n",
    "def train_logistic_regression(X, y, lr=0.01, epochs=5000):\n",
    "    n_samples = len(X)\n",
    "    n_features = len(X[0])\n",
    "    weights = [0] * n_features\n",
    "    bias = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        grad_w = [0] * n_features\n",
    "        grad_b = 0\n",
    "        for i in range(n_samples):\n",
    "            z = bias\n",
    "            for j in range(n_features):\n",
    "                z += weights[j] * X[i][j]\n",
    "            p = sigmoid(z)\n",
    "            error = p - y[i]\n",
    "            for j in range(n_features):\n",
    "                grad_w[j] += error * X[i][j]\n",
    "            grad_b += error\n",
    "        for j in range(n_features):\n",
    "            weights[j] -= lr * grad_w[j] / n_samples\n",
    "        bias -= lr * grad_b / n_samples\n",
    "\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fe6cc5-0222-4d7d-8755-091b181239f3",
   "metadata": {},
   "source": [
    "Again, a slightly modified **predict function** is required to determine the respective y values for the underlying x values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad2d0f35-1e82-4734-8e8d-b96820b46ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Prediction of Multivariate Logistic Regression\n",
    "def predict(x_input, weights, bias):\n",
    "    p = predict_proba(x_input, weights, bias)\n",
    "    return 1 if p >= 0.5 else 0, p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ccb60-abd3-47d3-8718-78b8c2d9e551",
   "metadata": {},
   "source": [
    "The **application examples** for the multiple logistic regression focus on weights and bias of the model and return logits and probabilities as values for classification. A classification example highlights the functionalty of multiple logistic regression models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a8c5c8d-6684-4a26-aa0e-1d092a2a9fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [4.6434554996184625, -0.6104014099036235]\n",
      "Intercept: -0.6030681183051588\n",
      "\n",
      "Logits and Probabilities:\n",
      "x = [8.3, 70] Logit = -4.790486164725556 P(y=1) = 0.008239982480800134\n",
      "x = [8.6, 65] Logit = -0.34544246532190825 P(y=1) = 0.4144881023070313\n",
      "x = [8.8, 63] Logit = 1.8040514544090351 P(y=1) = 0.8586412534285582\n",
      "x = [10.5, 72] Logit = 4.204313114627809 P(y=1) = 0.9852885767622177\n",
      "x = [10.7, 81] Logit = -0.3606084745811121 P(y=1) = 0.4108123380995226\n",
      "x = [10.8, 83] Logit = -1.1170657444265109 P(y=1) = 0.2465561029564774\n",
      "x = [11, 66] Logit = 10.188449323858778 P(y=1) = 0.9999623990060196\n",
      "x = [11, 75] Logit = 4.694836634726166 P(y=1) = 0.990940436398861\n",
      "x = [11.1, 80] Logit = 2.1071751351698973 P(y=1) = 0.8915984734384245\n",
      "x = [11.2, 75] Logit = 5.623527734649856 P(y=1) = 0.9964011082757194\n",
      "x = [11.3, 79] Logit = 3.6462676449972173 P(y=1) = 0.9745749171336164\n",
      "x = [11.4, 76] Logit = 5.941817424669929 P(y=1) = 0.9973796234329447\n",
      "x = [11.4, 76] Logit = 5.941817424669929 P(y=1) = 0.9973796234329447\n",
      "x = [11.7, 69] Logit = 11.607663943880832 P(y=1) = 0.99999090390386\n",
      "x = [12, 75] Logit = 9.338292134344636 P(y=1) = 0.9999120176066596\n",
      "x = [12.9, 74] Logit = 14.127803493904873 P(y=1) = 0.9999992682264455\n",
      "x = [12.9, 85] Logit = 7.413387984965014 P(y=1) = 0.999397236680573\n",
      "x = [13.3, 86] Logit = 8.660368774908777 P(y=1) = 0.9998267086465211\n",
      "x = [13.7, 71] Logit = 19.673772123310506 P(y=1) = 0.9999999971437574\n",
      "x = [13.8, 64] Logit = 24.41092754259772 P(y=1) = 0.9999999999749691\n",
      "x = [14, 78] Logit = 16.793998903870687 P(y=1) = 0.9999999491297548\n",
      "x = [14.2, 80] Logit = 16.501887183987122 P(y=1) = 0.9999999318719048\n",
      "x = [14.5, 74] Logit = 21.557332293294408 P(y=1) = 0.9999999995657141\n",
      "x = [16, 72] Logit = 29.74331836252935 P(y=1) = 0.999999999999879\n",
      "x = [16.3, 77] Logit = 28.08434796289678 P(y=1) = 0.9999999999993645\n",
      "x = [17.3, 81] Logit = 30.28619782290074 P(y=1) = 0.9999999999999296\n",
      "x = [17.5, 82] Logit = 30.604487512920812 P(y=1) = 0.9999999999999489\n",
      "x = [17.9, 80] Logit = 33.68267253257544 P(y=1) = 0.9999999999999976\n",
      "x = [18, 80] Logit = 34.147018082537286 P(y=1) = 0.9999999999999984\n",
      "x = [18, 80] Logit = 34.147018082537286 P(y=1) = 0.9999999999999984\n",
      "x = [20.6, 87] Logit = 41.947192512219935 P(y=1) = 1.0\n",
      "\n",
      "Predicted Class: (1, 0.9973796234329447)\n"
     ]
    }
   ],
   "source": [
    "# Application Examples\n",
    "weights, bias = train_logistic_regression(X, y)\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Intercept:\", bias)\n",
    "\n",
    "print(\"\\nLogits and Probabilities:\")\n",
    "for i in range(len(X)):\n",
    "    z = bias + sum([weights[j] * X[i][j] for j in range(len(weights))])\n",
    "    p = sigmoid(z)\n",
    "    print(\"x =\", X[i], \"Logit =\", z, \"P(y=1) =\", p)\n",
    "\n",
    "classification = predict([11.4, 76], weights, bias)\n",
    "print(\"\\nPredicted Class:\", classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be745d1-84c8-4610-916f-852a536aa5dc",
   "metadata": {},
   "source": [
    "## Machine Learning: Clustering\n",
    "### K-Means based upon Within Cluster Sum of Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddbf068-dd7c-4955-b762-04bde0d71420",
   "metadata": {},
   "source": [
    "Two variables and 15 cases of the original **trees dataset**, provided by Atkinson, A. C. (1985): *Plots, Transformations and Regression* via Oxford University Press. The other 15 cases are simulated trees, based upon another type of tree. Therefore, the dependant variable is dichotomized, indicating black cherry trees from the original dataset by 0 and simulated trees by 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6377f9c9-5fc0-47a6-a315-3aba5f69c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girth (x1), Height (x2) and Class (y) of Black Cherry Trees and Simulated Trees\n",
    "X = [\n",
    "    [8.3, 70], [8.6, 65], [8.8, 63], [10.5, 72], [10.7, 81], [10.8, 83], [11.0, 66], [11.0, 75], [11.1, 80],\n",
    "    [11.2, 75], [11.3, 79], [11.4, 76], [11.7, 69], [12.0, 75], [12.9, 74], [5.2, 45], [5.5, 48], [6.0, 50],\n",
    "    [6.3, 46], [6.7, 49], [7.0, 51], [7.2, 47], [7.4, 52], [7.5, 50], [7.7, 46], [7.9, 53], [8.1, 49],\n",
    "    [8.4, 47], [8.5, 54], [8.7, 52]\n",
    "]\n",
    "\n",
    "# y = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1 ,1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025152de-6ebb-4bd0-b2a4-9d2c2c74bd0e",
   "metadata": {},
   "source": [
    "The **euclidean distance** measures the straight-line distance between two points in a multi-dimensional space, calculated as the square root of the sum of the squared differences between corresponding coordinates. It’s commonly used in clustering and classification tasks to determine similarity between data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ad0e8ae-3f0f-4e18-8173-83915a0c5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Euclidean Distance\n",
    "def euclidean_distance(p1, p2):\n",
    "    return sum((p1[i] - p2[i])**2 for i in range(len(p1))) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095a1a3-c5f8-4815-974a-71f4d13246c4",
   "metadata": {},
   "source": [
    "The **initializing centroids function** sets the starting points for the cluster centers and influences the convergence of the algorithm and the quality of the final clusters, as it determines how the data is grouped during the iterative process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1910808-8f03-420f-9570-1a110796da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Centroids Function\n",
    "def initialize_centroids(X, k):\n",
    "    return [X[i][:] for i in range(k)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068c2b9d-3088-4b0a-817b-337ac249fa53",
   "metadata": {},
   "source": [
    "The **assigning clusters function** groups data points into clusters based on their proximity to the centroids. For each point, it calculates the euclidean distance to each centroid and assigns the point to the closest centroid’s cluster, ensuring that each cluster contains the points nearest to its respective centroid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffeaa203-e72b-4ed2-a077-d2725c17a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning Clusters Function\n",
    "def assign_clusters(X, centroids):\n",
    "    clusters = [[] for _ in centroids]\n",
    "    for point in X:\n",
    "        distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
    "        min_index = distances.index(min(distances))\n",
    "        clusters[min_index].append(point)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65757e-9a40-46e2-ba82-8aef7302cff9",
   "metadata": {},
   "source": [
    "The **computing centroids function** calculates the new centroids by finding the mean of all points within each cluster. For each cluster, it averages the values of each feature across all points, updating the centroid to represent the center of that cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48d36a37-1443-47d6-84b4-3524e5e114a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Centroids Function\n",
    "def compute_centroids(clusters):\n",
    "    new_centroids = []\n",
    "    for cluster in clusters:\n",
    "        if not cluster:\n",
    "            continue\n",
    "        n_features = len(cluster[0])\n",
    "        mean = [0] * n_features\n",
    "        for point in cluster:\n",
    "            for i in range(n_features):\n",
    "                mean[i] += point[i]\n",
    "        mean = [val / len(cluster) for val in mean]\n",
    "        new_centroids.append(mean)\n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ea304-224d-4673-a8fb-c1757cee4087",
   "metadata": {},
   "source": [
    "The **within cluster sum of squares** is defined as the total squared distance between each point and its assigned cluster centroid. It measures the compactness of the clusters, with smaller values indicating tighter clusters. The code computes this by summing the squared differences for all points in each cluster, relative to the centroid of that cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2045070-1148-4e3d-a83d-c9b766bc815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within Cluster Sum of Squares\n",
    "def wcss(clusters, centroids):\n",
    "    total = 0\n",
    "    for i in range(len(clusters)):\n",
    "        for point in clusters[i]:\n",
    "            total += sum((point[j] - centroids[i][j])**2 for j in range(len(point)))\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f52ee-4c50-4e47-9908-17f55b1ce15e",
   "metadata": {},
   "source": [
    "The **k-means algorithm** groups data points into k clusters. It iteratively assigns points to the closest centroids, recalculates the centroids, and computes the within cluster sum of squares until the centroids no longer change or the maximum number of iterations is reached, returning the final within cluster sum of squares value to assess the clustering quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26bf01b7-8537-48a2-a027-8fc91ef7199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Algorithm\n",
    "def kmeans_wcss(X, k=2, max_iter=100):\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    for _ in range(max_iter):\n",
    "        clusters = assign_clusters(X, centroids)\n",
    "        new_centroids = compute_centroids(clusters)\n",
    "        if new_centroids == centroids:\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "\n",
    "    labels = [0] * len(X)\n",
    "    for cluster_index, cluster_points in enumerate(clusters):\n",
    "        for point in cluster_points:\n",
    "            for idx, original_point in enumerate(X):\n",
    "                if original_point == point:\n",
    "                    labels[idx] = cluster_index\n",
    "                    break\n",
    "\n",
    "    return wcss(clusters, centroids), labels, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12459e55-0c51-4d2d-abb6-cb7e9b781f29",
   "metadata": {},
   "source": [
    "The **k-means indicator** highlights the chance of the within cluster sum of squares values when the number of centroids is increased. A decreasing value indicates a better allocation of the cases to the centroids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2aab3927-cf15-4914-b899-43470bf6f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Indicator\n",
    "def kmeans_indicator(X, max_k=10):\n",
    "    wcss_values = []\n",
    "    for k in range(1, max_k + 1):\n",
    "        wcss_value, _, _ = kmeans_wcss(X, k)\n",
    "        wcss_values.append(wcss_value)\n",
    "    return wcss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce3bff1-870c-47a5-9494-4fa1aec3201a",
   "metadata": {},
   "source": [
    "The **application examples** indicate the within cluster sum of squares for each number of clusters. In addition, it indicates the number of clusters within the dataset, which in this case is supposed to be 2 and assigns the labels accordingly. The position of the centroids is highlighted as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6dbbc3bb-c6d0-4ce3-83ae-eaca0fd849b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WCSS values for k = 1 to 6:\n",
      "k = 1 -> WCSS = 5162.787999999997\n",
      "k = 2 -> WCSS = 651.9133333333333\n",
      "k = 3 -> WCSS = 282.73966666666666\n",
      "k = 4 -> WCSS = 230.22711111111107\n",
      "k = 5 -> WCSS = 155.58683333333332\n",
      "k = 6 -> WCSS = 148.796\n",
      "\n",
      "WCSS: 651.9133333333333\n",
      "\n",
      "Cluster Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Centroids: [[10.753333333333334, 73.53333333333333], [7.206666666666667, 49.266666666666666]]\n"
     ]
    }
   ],
   "source": [
    "## Application examples\n",
    "wcss_list = kmeans_indicator(X, max_k=6)\n",
    "\n",
    "print(\"WCSS values for k = 1 to 6:\")\n",
    "for k, w in enumerate(wcss_list, 1):\n",
    "    print(\"k =\", k, \"-> WCSS =\", w)\n",
    "\n",
    "wcss_value, cluster_labels, final_centroids = kmeans_wcss(X, k=2)\n",
    "\n",
    "print(\"\\nWCSS:\", wcss_value)\n",
    "print(\"\\nCluster Labels:\", cluster_labels)\n",
    "print(\"\\nCentroids:\", final_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d27927-2434-4ca6-9bec-93ff6597d2ef",
   "metadata": {},
   "source": [
    "# Machine Learning: Dimensionality Reduction\n",
    "## Exploratory Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc57607-4fb3-477d-95d4-1e02bada8c93",
   "metadata": {},
   "source": [
    "These are 10 variables, based upon 5 variables each for the two personality dimensions extraversion and neuroticism, from the **bfi dataset** by Revelle, W., Wilt, J. and A. Rosenthal (2010): *Individual Differences in Cognition: New Methods for examining the Personality-Cognition Link* via Springer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dbddabab-f2b7-4f22-af4b-d9d4381d587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraversion (x1:x5) and Neuroticism (x6:x10) of the Big Five Inventory\n",
    "X = [\n",
    "    [2, 1, 6, 5, 6, 3, 5, 2, 2, 3],\n",
    "    [3, 6, 4, 2, 1, 6, 3, 2, 6, 4],\n",
    "    [1, 3, 2, 5, 4, 3, 3, 4, 2, 3],\n",
    "    [3, 4, 3, 6, 5, 2, 4, 2, 2, 3],\n",
    "    [2, 1, 2, 5, 2, 2, 2, 2, 2, 2],\n",
    "    [2, 2, 4, 6, 6, 4, 4, 4, 6, 6],\n",
    "    [3, 2, 5, 5, 6, 2, 3, 3, 1, 1],\n",
    "    [1, 1, 6, 6, 6, 2, 3, 1, 2, 1],\n",
    "    [2, 4, 4, 2, 6, 3, 3, 5, 3, 2],\n",
    "    [1, 2, 6, 5, 4, 1, 4, 2, 2, 5],\n",
    "    [1, 2, 6, 5, 5, 5, 4, 4, 3, 1],\n",
    "    [1, 2, 4, 5, 5, 3, 2, 4, 1, 2],\n",
    "    [6, 6, 2, 1, 1, 1, 2, 1, 3, 6],\n",
    "    [3, 4, 3, 2, 3, 5, 3, 4, 4, 3],\n",
    "    [6, 6, 3, 2, 2, 2, 2, 2, 4, 1],\n",
    "    [3, 4, 3, 3, 5, 5, 6, 5, 5, 4],\n",
    "    [3, 2, 3, 6, 5, 1, 2, 1, 2, 1],\n",
    "    [4, 3, 4, 4, 4, 2, 2, 3, 3, 3],\n",
    "    [3, 3, 2, 5, 4, 2, 3, 1, 3, 2],\n",
    "    [6, 4, 4, 4, 3, 2, 2, 3, 4, 5]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dae611-11a7-4742-9640-a930bf58ac94",
   "metadata": {},
   "source": [
    "Mean centering via a **mean center function**, is often used in data preprocessing to make the dataset more suitable for machine learning algorithms by ensuring all features contribute equally to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b721b47-c843-4ef7-a9ee-ab7a43822adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Center Function\n",
    "def mean_center(X):\n",
    "    cols = len(X[0])\n",
    "    rows = len(X)\n",
    "    means = [sum(X[i][j] for i in range(rows)) / rows for j in range(cols)]\n",
    "    centered = [[X[i][j] - means[j] for j in range(cols)] for i in range(rows)]\n",
    "    return centered, means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe74ba5-5d78-408e-9114-0ae46358a662",
   "metadata": {},
   "source": [
    "Again, the **correlation matrix** quantifies the strength and direction of the linear relationship between two variables. This code can be used to correlate several variables and summarize the corresponding values in one matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f79a7480-ba0a-4a0a-baed-c9b8336cbd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "def correlation_matrix(X):\n",
    "    rows = len(X)\n",
    "    cols = len(X[0])\n",
    "    corr = [[0]*cols for _ in range(cols)]\n",
    "\n",
    "    for i in range(cols):\n",
    "        for j in range(cols):\n",
    "            xi = [row[i] for row in X]\n",
    "            xj = [row[j] for row in X]\n",
    "            num = sum(xi[k] * xj[k] for k in range(rows))\n",
    "            denom_i = sum(xi[k]**2 for k in range(rows)) ** 0.5\n",
    "            denom_j = sum(xj[k]**2 for k in range(rows)) ** 0.5\n",
    "            corr[i][j] = num / (denom_i * denom_j)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f312c679-c3a6-4080-8a2e-1f886fbf3130",
   "metadata": {},
   "source": [
    "The **power iteration function** is an algorithm used to compute the dominant eigenvalues and eigenvectors of a matrix. The process involves iteratively applying matrix-vector multiplication to a random initial vector and normalizing it to avoid overflow or underflow, which allows the vector to converge to the eigenvector corresponding to the largest eigenvalue: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a22cf945-5b74-43db-8441-cb8f02485093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power Iteration Function \n",
    "def power_iteration(A, num_vectors=2, iterations=100):\n",
    "    n = len(A)\n",
    "    eigenvectors = []\n",
    "    eigenvalues = []\n",
    "\n",
    "    for _ in range(num_vectors):\n",
    "        b = [1.0]*n\n",
    "        for _ in range(iterations):\n",
    "            # Multiply A * b\n",
    "            Ab = [sum(A[i][j] * b[j] for j in range(n)) for i in range(n)]\n",
    "            norm = sum(x**2 for x in Ab) ** 0.5\n",
    "            b = [x / norm for x in Ab]\n",
    "        # Rayleigh quotient for eigenvalue\n",
    "        Ab = [sum(A[i][j] * b[j] for j in range(n)) for i in range(n)]\n",
    "        eigval = sum(b[i] * Ab[i] for i in range(n))\n",
    "        eigenvalues.append(eigval)\n",
    "        eigenvectors.append(b)\n",
    "\n",
    "        # Deflation\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                A[i][j] -= eigval * b[i] * b[j]\n",
    "\n",
    "    return eigenvalues, eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f8a0e2-d94d-4e0e-bf19-03d4bb530b33",
   "metadata": {},
   "source": [
    "The **factor loadings function** computes the factor loadings based on the correlation matrix, eigenvalues, and eigenvectors. In factor analysis, factor loadings represent the relationships between observed variables and the underlying latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2521b949-7c15-43eb-a159-fec7fce4fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor Loadings Function\n",
    "def factor_loadings(corr_matrix, eigenvalues, eigenvectors):\n",
    "    loadings = []\n",
    "    for i in range(len(corr_matrix)):\n",
    "        row = []\n",
    "        for j in range(len(eigenvectors)):\n",
    "            loading = eigenvectors[j][i] * (eigenvalues[j] ** 0.5)\n",
    "            row.append(loading)\n",
    "        loadings.append(row)\n",
    "    return loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bbb18a-86da-494e-86e1-5d9288c44459",
   "metadata": {},
   "source": [
    "This **application example** shows how to compute the correlation matrix, the eigenvalues as well as the corresponding factor loadings for identifying the underlying factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e727248-f8be-4e51-b488-ab9bea3ac908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation matrix:\n",
      "['1.00', '0.70', '-0.41', '-0.56', '-0.57', '-0.26', '-0.39', '-0.26', '0.33', '0.26']\n",
      "['0.70', '1.00', '-0.46', '-0.82', '-0.65', '0.19', '-0.18', '0.01', '0.54', '0.32']\n",
      "['-0.41', '-0.46', '1.00', '0.30', '0.51', '0.11', '0.35', '0.06', '-0.16', '-0.12']\n",
      "['-0.56', '-0.82', '0.30', '1.00', '0.62', '-0.27', '0.16', '-0.17', '-0.45', '-0.25']\n",
      "['-0.57', '-0.65', '0.51', '0.62', '1.00', '-0.01', '0.47', '0.33', '-0.33', '-0.30']\n",
      "['-0.26', '0.19', '0.11', '-0.27', '-0.01', '1.00', '0.46', '0.59', '0.62', '0.08']\n",
      "['-0.39', '-0.18', '0.35', '0.16', '0.47', '0.46', '1.00', '0.34', '0.23', '0.21']\n",
      "['-0.26', '0.01', '0.06', '-0.17', '0.33', '0.59', '0.34', '1.00', '0.24', '0.08']\n",
      "['0.33', '0.54', '-0.16', '-0.45', '-0.33', '0.62', '0.23', '0.24', '1.00', '0.51']\n",
      "['0.26', '0.32', '-0.12', '-0.25', '-0.30', '0.08', '0.21', '0.08', '0.51', '1.00']\n",
      "\n",
      "Eigenvalues:\n",
      "Factor 1 : 3.836\n",
      "Factor 2 : 2.552\n",
      "\n",
      "Factor Loadings:\n",
      "V1 : ['0.79', '-0.29']\n",
      "V2 : ['0.91', '0.11']\n",
      "V3 : ['-0.59', '0.25']\n",
      "V4 : ['-0.82', '-0.20']\n",
      "V5 : ['-0.83', '0.24']\n",
      "V6 : ['0.12', '0.87']\n",
      "V7 : ['-0.33', '0.72']\n",
      "V8 : ['-0.08', '0.73']\n",
      "V9 : ['0.59', '0.63']\n",
      "V10 : ['0.44', '0.32']\n"
     ]
    }
   ],
   "source": [
    "X_centered, means = mean_center(X)\n",
    "R = correlation_matrix(X_centered)\n",
    "eigvals, eigvecs = power_iteration([row[:] for row in R], num_vectors=2)\n",
    "loadings = factor_loadings(R, eigvals, eigvecs)\n",
    "\n",
    "print(\"Correlation matrix:\")\n",
    "for row in R:\n",
    "    print([\"{0:.2f}\".format(x) for x in row])\n",
    "\n",
    "print(\"\\nEigenvalues:\")\n",
    "for i, val in enumerate(eigvals):\n",
    "    print(\"Factor\", i+1, \":\", round(val, 3))\n",
    "\n",
    "print(\"\\nFactor Loadings:\")\n",
    "for i, row in enumerate(loadings):\n",
    "    print(\"V\" + str(i+1), \":\", [\"{0:.2f}\".format(x) for x in row])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dedb26-81bf-4b2f-97e1-580eb8eab74f",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Pretrained Neural Network (with Weights and Biases from TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242bfcd7-c77b-4f26-8b17-8d0e8851897e",
   "metadata": {},
   "source": [
    "These are five variables from the **iris dataset** by Fisher, R. (1936): *The use of multiple measurements in taxonomic problems* via John Wiley & Sons. The four independent variables are based upon length and width of the sepal leaf (x1 and x2) as well as the petal leaf (x3 and x4). All indipendent variables are standardized. Additionaly, the dependent variable differs between versicolor (0) and virginica (1) as different species of iris flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26ca5210-b794-48a9-a696-e831b573acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized independent variables (Xtest) and dichotomized dependent variable (ytrue)\n",
    "X = [[ 0.81575475, -0.21746808, -0.12904165, -0.65303909],\n",
    "         [ 0.05761837,  1.59476592,  0.84485761,  1.71304456],\n",
    "         [ 0.96738203,  0.68864892, -0.00730424, -0.41643072],\n",
    "         [ 2.02877297,  0.38660992,  2.06223168,  1.00321947],\n",
    "         [ 1.42226386,  0.99068792,  1.33180724,  0.29339437],\n",
    "         [ 0.81575475,  0.99068792,  1.21006983,  1.4764362 ],\n",
    "         [-1.00377258,  0.38660992, -0.49425387, -0.41643072],\n",
    "         [ 0.05761837, -0.51950708, -0.00730424,  0.29339437],\n",
    "         [ 0.36087292,  0.38660992,  1.08833242,  1.23982783],\n",
    "         [ 0.66412748,  0.38660992,  0.35790798,  1.4764362 ],\n",
    "         [ 0.05761837,  0.08457092,  0.84485761,  0.29339437],\n",
    "         [-0.70051802, -0.51950708,  0.23617057,  0.53000274],\n",
    "         [ 0.20924564, -0.21746808,  0.84485761,  1.00321947],\n",
    "         [-0.24563619,  0.08457092, -0.25077906, -0.65303909],\n",
    "         [-2.06516352, -1.42562408, -1.95510276, -1.59947255],\n",
    "         [-1.15539985, -1.42562408, -1.34641572, -1.36286418],\n",
    "         [ 0.05761837, -1.12358508, -0.00730424, -0.41643072],\n",
    "         [ 0.20924564,  0.08457092, -0.73772869, -0.88964745],\n",
    "         [-0.39726347, -0.51950708,  0.23617057, -0.17982236],\n",
    "         [ 0.5125002 ,  0.08457092, -0.37251647, -0.88964745]]\n",
    "\n",
    "y = [0,1,0,1,1,1,0,1,1,1,1,1,1,0,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3113d1-99fd-43b9-b873-b3b9e810f661",
   "metadata": {},
   "source": [
    "Normally all functions in MicroPython can be coded manually. However, the **math library** is imported here to simplify the execution the exponential function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e099137-6561-4b64-9540-45d18764b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bbbd7f-a922-4119-9fed-437f9af660ee",
   "metadata": {},
   "source": [
    "Neural networks are based upon neurons and **activation functions** decide whether a neuron should be activated or not. This means that it will decide whether the neuron's input to the neural network is important or not in the process of prediction using simpler mathematical operations like Rectified Linear Unit (ReLU), Leaky Rectified Linear Unit (Leaky ReLU), Hyperbolig Tangent (Tanh), Logistic Regression (Sigmoid) or Softmax (Softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d92f97ac-0b26-4d99-82b6-f8194d4f9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU\n",
    "def relu(x):\n",
    "    y = []\n",
    "    for i in range(len(x)):\n",
    "        if x[i] >= 0:\n",
    "            y.append(x[i])\n",
    "        else:\n",
    "            y.append(0)\n",
    "    return y\n",
    "\n",
    "# Leaky ReLU\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    p = []\n",
    "    for i in range(len(x)):\n",
    "        if x[i] >= 0:\n",
    "            p.append(x[i])\n",
    "        else:\n",
    "            p.append(alpha * x[i])\n",
    "    return p\n",
    "\n",
    "# Tanh\n",
    "def tanh(x):\n",
    "    t = [(math.exp(x[val]) - math.exp(-x[val])) / (math.exp(x[val]) + math.exp(-x[val])) for val in range(len(x))]\n",
    "    return t\n",
    "\n",
    "# Sigmoid\n",
    "def sigmoid(x):\n",
    "    z = [1 / (1 + math.exp(-x[val])) for val in range(len(x))]\n",
    "    return z\n",
    "\n",
    "# Softmax\n",
    "def softmax(x):\n",
    "    max_x = max(x[val])\n",
    "    exp_x = [math.exp(val - max_x) for val in range(len(x))]\n",
    "    sum_exp_x = sum(exp_x)\n",
    "    s = [j / sum_exp_x for j in exp_x]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fc163a-c1b2-47bc-b201-7e6c3da7ce41",
   "metadata": {},
   "source": [
    "A **single neuron** therefore accesses one of the previously defined activation functions and can be defined as follows in MicroPython:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "556c9d6c-79d7-4a60-9054-a5c2cba90040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Neuron\n",
    "def neuron(x, w, b, activation):\n",
    "\n",
    "    tmp = zero_dim(x[0])\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        tmp = add_dim(tmp, [(float(w[i]) * float(x[i][j])) for j in range(len(x[0]))])\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        yp = sigmoid([tmp[i] + b for i in range(len(tmp))])\n",
    "    elif activation == \"relu\":\n",
    "        yp = relu([tmp[i] + b for i in range(len(tmp))])\n",
    "    elif activation == \"leaky_relu\":\n",
    "        yp = relu([tmp[i] + b for i in range(len(tmp))])\n",
    "    elif activation == \"tanh\":\n",
    "        yp = tanh([tmp[i] + b for i in range(len(tmp))])\n",
    "    elif activation == \"softmax\":\n",
    "        yp = tanh([tmp[i] + b for i in range(len(tmp))])\n",
    "    else:\n",
    "        print(\"Function unknown!\")\n",
    "\n",
    "    return yp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60425596-dec2-4728-9aeb-1f4921f13821",
   "metadata": {},
   "source": [
    "In order for the data to be adequately processed by a neural network, a series of data formats such as vectors, matrices and the architecture of neural networks via layers must be defined. These **mathematical basics** of a neural network can be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0697d0ba-eadd-4d8c-97b8-58c47382945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - I\n",
    "def zero_dim(x):\n",
    "    z = [0 for i in range(len(x))]\n",
    "    return z\n",
    "\n",
    "# Mathematical Basics - II\n",
    "def add_dim(x, y):\n",
    "    z = [x[i] + y[i] for i in range(len(x))]\n",
    "    return z\n",
    "\n",
    "# Mathematical Basics - III\n",
    "def zeros(rows, cols):\n",
    "    M = []\n",
    "    while len(M) < rows:\n",
    "        M.append([])\n",
    "        while len(M[-1]) < cols:\n",
    "            M[-1].append(0.0)\n",
    "    return M\n",
    "\n",
    "# Mathematical Basics - IV\n",
    "def transpose(M):\n",
    "    if not isinstance(M[0], list):\n",
    "        M = [M]\n",
    "    rows = len(M)\n",
    "    cols = len(M[0])\n",
    "    MT = zeros(cols, rows)\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            MT[j][i] = M[i][j]\n",
    "    return MT\n",
    "\n",
    "# Mathematical Basics - V\n",
    "def print_matrix(M, decimals=3):\n",
    "    for row in M:\n",
    "        print([round(x, decimals) + 0 for x in row])\n",
    "\n",
    "# Mathematical Basics - VI\n",
    "def dense(nunit, x, w, b, activation):\n",
    "    res = []\n",
    "    for i in range(nunit):\n",
    "        z = neuron(x, w[i], b[i], activation)\n",
    "        res.append(z)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42692483-6bc9-457a-ac0a-83ed25f8739f",
   "metadata": {},
   "source": [
    "The architecture of a neural network can be reconstructed in MicroPython with the **weights and biases** from a already pretrained deep learning model. They can be transferred from TensorFlow (which is a deep learning library suitable for Python) to MicroPython. The following structure indicates four independent variables (rows) for two neurons (columns) in the input layer with the according weight w1. In addition, the first layer has two accoring biases b1. Therefore, the first hidden layer consists of three neurons with w2 and b2, the second hidden layer consists of two neurons with w3 and b3 and the output layer is a single neuron with w4 and b4. As a result, this neural network consists of a total of eight neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e132c98-e540-4276-8285-f039bf2f6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include Parameters from TensorFlow\n",
    "w1 = [[-0.75323504, -0.25906014],\n",
    "      [-0.46379513, -0.5019245 ],\n",
    "      [ 2.1273055 ,  1.7724446 ],\n",
    "      [ 1.1853403 ,  0.88468695]]\n",
    "b1 = [0.53405946, 0.32578036]\n",
    "w2 = [[-1.6785783,  2.0158117,  1.2769054],\n",
    "      [-1.4055765,  0.6828738,  1.5902631]]\n",
    "b2 = [ 1.18362  , -1.1555661, -1.0966455]\n",
    "w3 = [[ 0.729278  , -1.0240695 ],\n",
    "      [-0.80972326,  1.4383037 ],\n",
    "      [-0.90892404,  1.6760625 ]]\n",
    "b3 = [0.10695826, 0.01635581]\n",
    "w4 = [[-0.2019448],\n",
    "      [ 1.5772797]]\n",
    "b4 = [-1.2177287]\n",
    "\n",
    "# Transpose\n",
    "w1 = transpose(w1)\n",
    "w2 = transpose(w2)\n",
    "w3 = transpose(w3)\n",
    "w4 = transpose(w4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a71df6-88bd-479f-9ac8-f6a7c4626c33",
   "metadata": {},
   "source": [
    "According to the transferred weights and biases the **architecture of the neural network** can be defined in MicroPython as follows. This specifies the number of neurons within each layer and the activation functions for activating the neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2173fd61-3840-40ab-a11f-bece19bfedb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.21977697810066976, 0.9762814497719644, 0.21977697810066976, 0.9763437580183316, 0.9752601081958471, 0.9763223651158681, 0.21977697810066976, 0.9324981536051185, 0.9763252950730309, 0.975354442588011, 0.9757636857966179, 0.975140989570476, 0.9762884065437853, 0.21977697810066976, 0.21977697810066976, 0.21977697810066976, 0.7158511603338569, 0.21977697810066976, 0.9567955464832789, 0.21977697810066976]]\n"
     ]
    }
   ],
   "source": [
    "# Neural Network Architecture\n",
    "yout1 = dense(2, transpose(X), w1, b1, 'relu') # input layer (2 neurons)\n",
    "yout2 = dense(3, yout1, w2, b2, 'sigmoid') # hidden layer (3 neurons)\n",
    "yout3 = dense(2, yout2, w3, b3, 'relu') # hidden layer (2 neurons)\n",
    "ypred = dense(1, yout3, w4, b4,'sigmoid') # output layer (1 neuron)\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38bc20-35f6-43fd-ba9e-ad19e09ad9f9",
   "metadata": {},
   "source": [
    "A **confusion matrix**, also known as an error matrix, is a table that visualizes the performance of a classification model by comparing its predictions against the actual results. It's a two-dimensional matrix that displays the counts of true positives, true negatives, false positives, and false negatives, providing a detailed view of where a model's predictions are correct and where it's making errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "485790f0-ecbe-4287-97d3-dc26d1883f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Basics\n",
    "def classification_report(y, ypred):\n",
    "    TP = TN = FP = FN = 0\n",
    "    for true, pred in zip(y, ypred):\n",
    "        if true == pred:\n",
    "            if true == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "        else:\n",
    "            if true == 1:\n",
    "                FN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    accuracy = (TP + TN) / len(y)\n",
    "    print(\"Accuracy: {:.3f}\".format(accuracy))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"TN: {}, FP: {}\".format(TN, FP))\n",
    "    print(\"FN: {}, TP: {}\".format(FN, TP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2863a37-4578-4865-820d-9c2bf481690b",
   "metadata": {},
   "source": [
    "The **performance of the pretrained neural network** can be viewed via the following MicroPython code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9fff625e-3e57-4fdb-924b-7cc7024deb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0]\n",
      "Accuracy: 0.900\n",
      "Confusion Matrix:\n",
      "TN: 8, FP: 2\n",
      "FN: 0, TP: 10\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "ypred_class = [1 if i > 0.5 else 0 for i in ypred[0]]\n",
    "print(ypred_class)\n",
    "print(classification_report(y, ypred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e03b2f-b797-40bb-a800-bacffeeacfd7",
   "metadata": {},
   "source": [
    "## Self-Learning Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbac749-739f-419e-8e7b-a2eec8b8625f",
   "metadata": {},
   "source": [
    "Again, the five variables from the **iris dataset** by Fisher, R. (1936): *The use of multiple measurements in taxonomic problems* via John Wiley & Sons will be used. The four independent variables are based upon length and width of the sepal leaf (x1 and x2) as well as the petal leaf (x3 and x4). All indipendent variables are standardized. Additionaly, the dependent variable differs between versicolor (0) and virginica (1) as different species of iris flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9b269792-bcad-46f9-a3a3-656fa0a09a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized independent variables (Xtest) and dichotomized dependent variable (ytrue)\n",
    "X = [[ 0.81575475, -0.21746808, -0.12904165, -0.65303909],\n",
    "         [ 0.05761837,  1.59476592,  0.84485761,  1.71304456],\n",
    "         [ 0.96738203,  0.68864892, -0.00730424, -0.41643072],\n",
    "         [ 2.02877297,  0.38660992,  2.06223168,  1.00321947],\n",
    "         [ 1.42226386,  0.99068792,  1.33180724,  0.29339437],\n",
    "         [ 0.81575475,  0.99068792,  1.21006983,  1.4764362 ],\n",
    "         [-1.00377258,  0.38660992, -0.49425387, -0.41643072],\n",
    "         [ 0.05761837, -0.51950708, -0.00730424,  0.29339437],\n",
    "         [ 0.36087292,  0.38660992,  1.08833242,  1.23982783],\n",
    "         [ 0.66412748,  0.38660992,  0.35790798,  1.4764362 ],\n",
    "         [ 0.05761837,  0.08457092,  0.84485761,  0.29339437],\n",
    "         [-0.70051802, -0.51950708,  0.23617057,  0.53000274],\n",
    "         [ 0.20924564, -0.21746808,  0.84485761,  1.00321947],\n",
    "         [-0.24563619,  0.08457092, -0.25077906, -0.65303909],\n",
    "         [-2.06516352, -1.42562408, -1.95510276, -1.59947255],\n",
    "         [-1.15539985, -1.42562408, -1.34641572, -1.36286418],\n",
    "         [ 0.05761837, -1.12358508, -0.00730424, -0.41643072],\n",
    "         [ 0.20924564,  0.08457092, -0.73772869, -0.88964745],\n",
    "         [-0.39726347, -0.51950708,  0.23617057, -0.17982236],\n",
    "         [ 0.5125002 ,  0.08457092, -0.37251647, -0.88964745]]\n",
    "\n",
    "y = [0,1,0,1,1,1,0,1,1,1,1,1,1,0,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b22af1-94e2-44e7-b60c-9cee236941ed",
   "metadata": {},
   "source": [
    "The **random library** and **math library** are imported to simplify the execution of some functions required for self-learning neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b13d33a-eedd-4fda-868f-c4b972d8b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc1b594-dc1b-4ad6-a907-50eca27f6330",
   "metadata": {},
   "source": [
    "Self-learning neural networks not only require **activation functions**, but also their derivates. The derivative of a function represents its instantaneous rate of change at a specific point. This allows the neural network to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a69231f-5ec6-4bc3-b3b0-7c4543f3bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# Derivate of Sigmoid\n",
    "def sigmoid_derivative(output):\n",
    "    return output * (1 - output)\n",
    "\n",
    "# ReLU\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "# Derivate of ReLU\n",
    "def relu_derivative(output):\n",
    "    return 1 if output > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a292fc-aaf2-4494-b94d-150613969bb4",
   "metadata": {},
   "source": [
    "Since the neural network is supposed to learn the weights and biases by itself, the layers and neurons of the neural network will be **initialized with some random values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "249982aa-8ef6-48d8-98e6-0df97bf3ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Initializing Weights and Biases\n",
    "def init_layer(input_size, output_size):\n",
    "    weights = [[random.uniform(-0.5, 0.5) for _ in range(input_size)] for _ in range(output_size)]\n",
    "    biases = [random.uniform(-0.5, 0.5) for _ in range(output_size)]\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ff0879-314e-4169-811f-a1f2ac51cd29",
   "metadata": {},
   "source": [
    "In neural networks, **forward propagation** is the process of passing input data through the network's layers to generate a prediction and **backward propagation**, on the other hand, is the mechanism used to train the network by calculating the error between the prediction and the actual output, and then adjusting the network's weights to minimize that error. This important for the learning ability of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3945ce4-cc5b-4a69-91c8-95894225536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Propagation\n",
    "def dense_forward(inputs, weights, biases, activation='relu'):\n",
    "    outputs = []\n",
    "    pre_activations = []\n",
    "    for w, b in zip(weights, biases):\n",
    "        z = sum(i*w_ij for i, w_ij in zip(inputs, w)) + b\n",
    "        pre_activations.append(z)\n",
    "        if activation == 'sigmoid':\n",
    "            outputs.append(sigmoid(z))\n",
    "        elif activation == 'relu':\n",
    "            outputs.append(relu(z))\n",
    "        else:\n",
    "            raise Exception(\"Unknown activation\")\n",
    "    return outputs, pre_activations\n",
    "\n",
    "# Backward Propagation\n",
    "def dense_backward(inputs, grad_outputs, outputs, pre_activations, weights, biases, activation='relu', lr=0.01):\n",
    "    input_grads = [0.0 for _ in range(len(inputs))]\n",
    "    for j in range(len(weights)):\n",
    "        if activation == 'sigmoid':\n",
    "            delta = grad_outputs[j] * sigmoid_derivative(outputs[j])\n",
    "        elif activation == 'relu':\n",
    "            delta = grad_outputs[j] * relu_derivative(pre_activations[j])\n",
    "        else:\n",
    "            raise Exception(\"Unknown activation\")\n",
    "        for i in range(len(inputs)):\n",
    "            input_grads[i] += weights[j][i] * delta\n",
    "            weights[j][i] -= lr * delta * inputs[i]\n",
    "        biases[j] -= lr * delta\n",
    "    return input_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe56a4-2ba9-48bc-9217-6e6cf96f3e92",
   "metadata": {},
   "source": [
    "Furthermore, a **loss function** quantifies the difference between a deep learning model's prediction and the actual outcome, essentially acting as a measure of the model's error. Cross-entropy, a specific type of loss function, is commonly used for classification problems, especially when the model outputs probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "007916f2-445d-4676-a9c8-1bfe1a18791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "def binary_cross_entropy(predicted, target):\n",
    "    epsilon = 1e-7\n",
    "    return - (target * math.log(predicted + epsilon) + (1 - target) * math.log(1 - predicted + epsilon))\n",
    "\n",
    "def binary_cross_entropy_derivative(predicted, target):\n",
    "    epsilon = 1e-7\n",
    "    return -(target / (predicted + epsilon)) + (1 - target) / (1 - predicted + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e1f48-8157-4260-ae43-c4d8cb05fc8d",
   "metadata": {},
   "source": [
    "This time the **architecture of the neural network** consists of four independent variables which will be forwarded to three neurons in the input layer and one neuron in the output layer. This is a very simple neural network that consists of four neurons in two layers with according weights (w1 and w2) and biases (b1 and b2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c10476c1-ab4a-4e1e-b447-371c140fcf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights and Biases\n",
    "w1, b1 = init_layer(4, 3)\n",
    "w2, b2 = init_layer(3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b4e87-26f5-44f4-a4a4-5d410e7f9b8b",
   "metadata": {},
   "source": [
    "Finally, the number of **epochs and the learning rate** need to be specified in MicriPython. In neural networks, an epoch represents one complete pass of the entire training dataset through the model. Learning rate determines how much the model's weights are adjusted during each update step in the training process. Both are crucial hyperparameters that influence training and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "55bc95cf-de09-46ea-9ca1-d43951b2dc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 13.6811\n",
      "Epoch 11, Loss: 5.7964\n",
      "Epoch 21, Loss: 2.9631\n",
      "Epoch 31, Loss: 1.8962\n",
      "Epoch 41, Loss: 1.3641\n",
      "Epoch 51, Loss: 1.0455\n",
      "Epoch 61, Loss: 0.8331\n",
      "Epoch 71, Loss: 0.6832\n",
      "Epoch 81, Loss: 0.5741\n",
      "Epoch 91, Loss: 0.4921\n",
      "Epoch 100, Loss: 0.4341\n"
     ]
    }
   ],
   "source": [
    "# Epochs and Learning Rate for Training\n",
    "epochs = 100\n",
    "lr = 0.05\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for xi, yi in zip(X, y):\n",
    "        # Forward pass\n",
    "        out1, pre1 = dense_forward(xi, w1, b1, 'relu')\n",
    "        out2, pre2 = dense_forward(out1, w2, b2, 'sigmoid')\n",
    "        loss = binary_cross_entropy(out2[0], yi)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Backward pass\n",
    "        dL_dout2 = [binary_cross_entropy_derivative(out2[0], yi)]\n",
    "        dL_dout1 = dense_backward(out1, dL_dout2, out2, pre2, w2, b2, 'sigmoid', lr)\n",
    "        _ = dense_backward(xi, dL_dout1, out1, pre1, w1, b1, 'relu', lr)\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ce189-fcc6-4b34-873e-0b28505cdc4c",
   "metadata": {},
   "source": [
    "The **outcome of the neural network** can be predicted with the collowing code in MicroPython:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5afa99e-1986-49e0-9cd2-409ecf616865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    out1, _ = dense_forward(x, w1, b1, 'relu')\n",
    "    out2, _ = dense_forward(out1, w2, b2, 'sigmoid')\n",
    "    return 1 if out2[0] > 0.5 else 0\n",
    "\n",
    "ypred = [predict(xi) for xi in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c059ff06-98ec-4042-a97f-09581296e10b",
   "metadata": {},
   "source": [
    "As in the pretrained neural network before, a **confusion matrix** can be used to evaluate the performance of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11da4a33-48e5-476d-b37a-a5e6e48ec602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report(ytrue, ypred):\n",
    "    TP = TN = FP = FN = 0\n",
    "    for true, pred in zip(ytrue, ypred):\n",
    "        if true == pred:\n",
    "            if true == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "        else:\n",
    "            if true == 1:\n",
    "                FN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    accuracy = (TP + TN) / len(ytrue)\n",
    "    print(\"Accuracy: {:.3f}\".format(accuracy))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"TN: {}, FP: {}\".format(TN, FP))\n",
    "    print(\"FN: {}, TP: {}\".format(FN, TP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c0404-d36f-43b4-8f5c-594a45fe6fa0",
   "metadata": {},
   "source": [
    "Finally, the **performance of the neural network** can be inspected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "32279623-c72c-4990-86b3-dbba92d9851f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000\n",
      "Confusion Matrix:\n",
      "TN: 10, FP: 0\n",
      "FN: 0, TP: 10\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "ypred = [predict(xi) for xi in X]\n",
    "\n",
    "# Show classification metrics\n",
    "classification_report(y, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8834a104-ab83-450a-8bce-f70b47884bda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
