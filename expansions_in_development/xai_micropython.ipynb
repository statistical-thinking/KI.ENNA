{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc2b15b0-cfbb-4413-a54e-ea07f37328a9",
   "metadata": {},
   "source": [
    "# Explainable Artificial Intelligence with MicroPython\n",
    "#### A comprehensive codebook by Prof. Dr. habil. Dennis Klinkhammer (2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125254bb-f917-498f-a4bb-44c40c79fe09",
   "metadata": {},
   "source": [
    "![title](img/logo.png)\n",
    "Explainable Artificial Intelligence (XAI) refers to a set of **methods and practices aimed at making the behavior and decisions of artificial intelligence systems understandable** to humans. As machine learning and deep learning models have grown in complexity, the need to demystify their inner workings has become increasingly important - especially in domains where decisions impact lives, such as healthcare, law, and public administration. XAI seeks to **provide transparency, interpretability, and trust** by enabling users to comprehend why a model made a certain prediction, how it weighs input data, and to what extent it can be held accountable for its outcomes.\n",
    "\n",
    "Traditionally, many machine learning models - especially those involving deep neural networks - function as \"black boxes,\" offering **high predictive accuracy but little insight into how they arrive at specific decisions**. Explainability challenges this paradigm by emphasizing models whose mechanisms can be logically traced, mathematically analyzed, and, ideally, intuitively understood by both developers and domain experts.\n",
    "\n",
    "In this context, **MicroPython** offers a uniquely advantageous environment for fostering explainability. As a **lightweight implementation of Python** designed for microcontrollers, MicroPython encourages **minimalism, clarity, and a hands-on approach** to programming. When complex artificial intelligence architectures are rebuilt in MicroPython from the ground up - without relying on abstracted libraries or opaque function calls - the result is a **codebase that closely mirrors the mathematical logic** behind machine learning and neural network operations. This stripped-down setting compels the developer to engage directly with core principles such as matrix multiplication, activation functions, and gradient descent, making the implementation not just visible, but **pedagogically powerful**.\n",
    "\n",
    "By removing the comfort of high-level libraries, MicroPython invites learners and researchers to **reengage with the fundamentals**. Each neuron, each layer, and each weight update must be explicitly defined and managed, thereby providing a unique opportunity to demystify how models process data, adjust parameters, and converge toward solutions. This low-level perspective is not only educational but also instrumental in aligning artificial intelligence development with the ideals of transparency and accountability central to XAI. Thus, combining the discipline of MicroPython with the goals of XAI creates a framework that is as instructive as it is principled - offering a rare and **valuable bridge between theoretical understanding and practical implementation**. Therefore, this comprehensive codebook covers a well chosen set of common statistical basics, as well as machine learning and deep learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be470f7-97f5-410b-8d3f-216aadf85cf8",
   "metadata": {},
   "source": [
    "# 1. Statistical Basics - I\n",
    "![title](img/one.png)\n",
    "We will start with some statistical basics: Mean, variance and standard deviation. As part of **univariate statistics**, they not only serve to **describe individual variables**, but are also important foundations for advanced statistical analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff51e3-2da4-4bcc-bdaa-a6f7902f24ba",
   "metadata": {},
   "source": [
    "## 1.1. Dataset\n",
    "One variable of the **trees dataset**, provided by Atkinson, A. C. (1985): *Plots, Transformations and Regression* via Oxford University Press:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b29baa-231c-4524-87e1-81bf021c5ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girth (x) of Black Cherry Trees\n",
    "x = [8.3, 8.6, 8.8, 10.5, 10.7, 10.8, 11, 11, 11.1, 11.2, 11.3, \n",
    "     11.4, 11.4, 11.7, 12, 12.9, 12.9, 13.3, 13.7, 13.8, 14, \n",
    "     14.2, 14.5, 16, 16.3, 17.3, 17.5, 17.9, 18, 18, 20.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac4456c-dff9-4870-900c-5b80bd1e2a31",
   "metadata": {},
   "source": [
    "## 1.2. Mean\n",
    "The **mean**, also known as the arithmetic mean, is one of the most common measures of central tendency in statistics. It represents the average value of a dataset and provides a single value that summarizes the entire data distribution. To calculate the mean, you sum all values in your dataset and divide this total by the number of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa1ddf-153d-4e35-8771-84cba0094232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "def mean(data):\n",
    "    return sum(data) / len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a79a9f-1a83-4e53-b343-7c703d0a2a8a",
   "metadata": {},
   "source": [
    "## 1.3. Variance\n",
    "The **sample variance** is a measure of how spread out the values in a dataset are. It quantifies the average squared deviation from the mean, giving insight into the variability within the sample. Unlike population variance, it divides by n−1n−1 to account for the degrees of freedom, making it an unbiased estimator when working with a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc871ff2-d1ca-4f37-af29-cf17162483e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance\n",
    "def variance(data):\n",
    "    m = mean(data)\n",
    "    return sum((x - m) ** 2 for x in data) / (len(data) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43581b1-8b0d-4af6-af57-76e635fad585",
   "metadata": {},
   "source": [
    "## 1.4. Standard Deviation\n",
    "The **standard deviation** is the square root of the variance and provides a measure of spread in the same units as the original data. It indicates how much the values in a dataset typically deviate from the mean, making it easier to interpret than variance in practical terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6840eb-173c-476a-a44a-3399da095331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Deviation\n",
    "def std_dev(data):\n",
    "    return variance(data) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf403c-e788-4fab-9402-a7f57d0c52c1",
   "metadata": {},
   "source": [
    "## 1.5. Application\n",
    "These are **application examples** for mean, sample variance and standard deviation in MicroPython:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8751d41-526d-422d-b8b6-0e661d195bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application Examples\n",
    "print(\"Mean\", mean(x))\n",
    "print(\"Variance\", variance(x))\n",
    "print(\"Standard Deviation\", std_dev(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e60abe2-36a6-4701-a2a0-0949bb6f4fe2",
   "metadata": {},
   "source": [
    "# 2. Statistical Basics - II\n",
    "![title](img/two.png)\n",
    "After analyzing individual variables, the focus is now on the possible **interactions between two variables**. The statistical basis for this is called **bivariate statistics**. Common methods include covariance, correlation and simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ae0cf-1dd9-4f0c-a116-aef473462e82",
   "metadata": {},
   "source": [
    "## 2.2. Dataset\n",
    "Two variables of the **trees dataset**, provided by Atkinson, A. C. (1985): *Plots, Transformations and Regression* via Oxford University Press:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c060aa37-2c26-41ad-ba66-f32571bf99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girth (x) and Volume (y) of Black Cherry Trees\n",
    "x = [8.3, 8.6, 8.8, 10.5, 10.7, 10.8, 11, 11, 11.1, 11.2, 11.3, \n",
    "     11.4, 11.4, 11.7, 12, 12.9, 12.9, 13.3, 13.7, 13.8, 14, \n",
    "     14.2, 14.5, 16, 16.3, 17.3, 17.5, 17.9, 18, 18, 20.6]\n",
    "\n",
    "y = [10.3, 10.3, 10.2, 16.4, 18.8, 19.7, 15.6, 18.2, 22.6, 19.9,\n",
    "     24.2, 21, 21.4, 21.3, 19.1, 22.2, 33.8, 27.4, 25.7, 24.9,\n",
    "     34.5, 31.7, 36.3, 38.3, 42.6, 55.4, 55.7, 58.3, 51.5, 51, 77]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6c3e0-0b50-4f89-b7d5-40a1ae73a2d5",
   "metadata": {},
   "source": [
    "## 2.3. Covariance\n",
    "The **covariance** measures the directional relationship between two variables. A positive covariance indicates that the variables tend to increase together, while a negative covariance suggests that as one increases, the other tends to decrease. It's a foundational concept in statistics for understanding how two variables vary together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c002c-fdcf-4f53-b93c-c1b029e1f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance\n",
    "def covariance(x, y):\n",
    "    mx = mean(x)\n",
    "    my = mean(y)\n",
    "    return sum((x[i] - mx) * (y[i] - my) for i in range(len(x))) / (len(x) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25314fc-07eb-4d07-b91f-082a98d6f6c5",
   "metadata": {},
   "source": [
    "## 2.4. Correlation\n",
    "The **correlation** quantifies the strength and direction of the linear relationship between two variables. It standardizes the covariance by dividing it by the product of the standard deviations, resulting in a value between -1 and 1. A correlation close to 1 or -1 indicates a strong relationship, while a value near 0 suggests little to no linear association:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed232c0a-8090-41e9-9535-474fdb9460de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation\n",
    "def correlation(x, y):\n",
    "    return covariance(x, y) / (std_dev(x) * std_dev(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b311e-8071-4cd7-9911-0b5270333c03",
   "metadata": {},
   "source": [
    "## 2.5. Single Linear Regression\n",
    "A **single linear regression** models the relationship between two variables by **fitting a straight line to the data**. It calculates the slope b and intercept a of the line y=a+bx, where b indicates how much y changes for each unit increase in x, and a is the predicted value of y when x=0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a5cb22-a7e8-4c5f-9c35-072c089f9bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression\n",
    "def linear_regression(x, y):\n",
    "    b = covariance(x, y) / variance(x)\n",
    "    a = mean(y) - b * mean(x)\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1869c6f-9bb5-42cd-99ee-e8dc58793030",
   "metadata": {},
   "source": [
    "## 2.6. Predict Function\n",
    "The **predict function** is required to **determine the respective y values for the underlying x values** via a and b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5932f8-4db3-49e6-87d8-9f21391c09aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Function\n",
    "def predict(x_new, a, b):\n",
    "    return a + b * x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8feef56-f685-41e5-acb4-6d4541cc0af6",
   "metadata": {},
   "source": [
    "## 2.7. Residuals\n",
    "**Residuals** represent the **differences between the observed values and the predicted values** from a linear regression model. They indicate **how well the model fits** the data: a residual close to 0 means a good fit, while larger residuals suggest that the model doesn't capture the data as accurately. The residuals can be used to assess the assumptions of linear regression and identify any outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05fb58-7484-47bd-aff5-4edff4e75761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals\n",
    "def residuals(x, y, a, b):\n",
    "    return [y[i] - (a + b * x[i]) for i in range(len(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832348e8-a7ce-4c05-b99b-e4164dfda8ec",
   "metadata": {},
   "source": [
    "## 2.8. Coefficient of Determination\n",
    "The **coefficient of determination** measures the proportion of **variance in the dependent variable that is explained by the independent variable** in a regression model. It indicates the goodness of fit: an coefficient of determination close to 1 means that the model explains most of the variance, while a value near 0 suggests the model doesn’t capture much of the variability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac66da-0f53-4b7b-9c71-cdaccd777386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient of Determination\n",
    "def r_squared(x, y, a, b):\n",
    "    y_mean = mean(y)\n",
    "    ss_tot = sum((yi - y_mean) ** 2 for yi in y)\n",
    "    ss_res = sum((y[i] - (a + b * x[i])) ** 2 for i in range(len(y)))\n",
    "    return 1 - ss_res / ss_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acdbc67-fbec-4df0-8849-6ee1d595c772",
   "metadata": {},
   "source": [
    "## 2.9. Application\n",
    "These are **application examples** for covariance, correlation, as well as the single linear regression with the coresponding predictions, residuals and the coefficient of determination in MicroPython:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4006c98-62e9-4ba2-a6b4-b7954d04c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apllication Examples\n",
    "print(\"Covariance:\", covariance(x, y))\n",
    "print(\"Correlation:\", correlation(x, y))\n",
    "\n",
    "a, b = linear_regression(x, y)\n",
    "print(\"\\nSingle Linear Regression: y = {:.2f} + {:.2f} * x\".format(a, b))\n",
    "print(\"Predictions for x = 11.4:\", predict(11.4, a, b))\n",
    "print(\"\\nResiduals:\", residuals(x, y, a, b))\n",
    "print(\"\\nCoefficient of Determination:\", r_squared(x, y, a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec27fa0-b62e-4b45-89a9-4e60d41bd58d",
   "metadata": {},
   "source": [
    "# 3. Machine Learning - I (Regression)\n",
    "![title](img/three.png)\n",
    "Because dependent variables are generally not dependent on just one independent variable, it is advisable to broaden the perspective to include **multivariate statistics** which can take several independent variables into account. Therefore, multiple linear regression is introduced as fist multivariate approach for **regression** tasks and in order to **predict the outcome** of the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bddd3c7-1fa2-44e6-83e7-37af516de606",
   "metadata": {},
   "source": [
    "## 3.1. Dataset\n",
    "Three variables of the **trees dataset**, provided by Atkinson, A. C. (1985): *Plots, Transformations and Regression* via Oxford University Press:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b1a9d-be90-4639-9fe2-1c8fb20915a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girth (x1), Height (x2) and Volume (y) of Black Cherry Trees \n",
    "X = [[8.3, 70], [8.6, 65], [8.8, 63], [10.5, 72], [10.7, 81], [10.8, 83], [11, 66], [11, 75], [11.1, 80], [11.2, 75],\n",
    "    [11.3, 79], [11.4, 76], [11.4, 76], [11.7, 69], [12, 75], [12.9, 74], [12.9, 85], [13.3, 86], [13.7, 71], [13.8, 64],\n",
    "    [14, 78], [14.2, 80], [14.5, 74], [16, 72], [16.3, 77], [17.3, 81], [17.5, 82], [17.9, 80], [18, 80], [18, 80], [20.6, 87]]\n",
    "\n",
    "y = [10.3, 10.3, 10.2, 16.4, 18.8, 19.7, 15.6, 18.2, 22.6, 19.9,\n",
    "     24.2, 21, 21.4, 21.3, 19.1, 22.2, 33.8, 27.4, 25.7, 24.9,\n",
    "     34.5, 31.7, 36.3, 38.3, 42.6, 55.4, 55.7, 58.3, 51.5, 51, 77]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ea49aa-3b27-401f-9e16-6e429235a431",
   "metadata": {},
   "source": [
    "## 3.2. Matrix Inversion\n",
    "**Matrix inversion** is essential in **solving systems of linear equations**, particularly in methods like multiple linear regression. The following code implements the **Gaussian elimination method** to invert a matrix, ensuring it is invertible by checking for non-zero pivots during the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee2fb4-0a5d-4133-bcef-a461c8bcface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Matrix Inversion\n",
    "def invert_matrix(matrix):\n",
    "    n = len(matrix)\n",
    "    identity = [[float(i == j) for j in range(n)] for i in range(n)]\n",
    "    m = [row[:] for row in matrix]\n",
    "\n",
    "    for i in range(n):\n",
    "        max_row = i\n",
    "        max_val = abs(m[i][i])\n",
    "        for k in range(i + 1, n):\n",
    "            if abs(m[k][i]) > max_val:\n",
    "                max_val = abs(m[k][i])\n",
    "                max_row = k\n",
    "\n",
    "        if max_val == 0:\n",
    "            raise ValueError(\"Matrix is not invertible!\")\n",
    "\n",
    "        if max_row != i:\n",
    "            m[i], m[max_row] = m[max_row], m[i]\n",
    "            identity[i], identity[max_row] = identity[max_row], identity[i]\n",
    "\n",
    "        factor = m[i][i]\n",
    "        for j in range(n):\n",
    "            m[i][j] /= factor\n",
    "            identity[i][j] /= factor\n",
    "\n",
    "        for k in range(n):\n",
    "            if k != i:\n",
    "                factor = m[k][i]\n",
    "                for j in range(n):\n",
    "                    m[k][j] -= factor * m[i][j]\n",
    "                    identity[k][j] -= factor * identity[i][j]\n",
    "\n",
    "    return identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b855e19-bdb2-49d5-a101-c3e11386d9e6",
   "metadata": {},
   "source": [
    "## 3.3. Matrix Transposition\n",
    "**Matrix transposition** involves **flipping a matrix over its diagonal**, converting rows into columns and vice versa. The resulting matrix is called the transpose of the original matrix. Transposition is commonly used in **linear algebra**, especially in operations like solving systems of equations or adjusting data representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0959f-336b-46e7-a90c-d8696bf7d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Matrix Transposition\n",
    "def transpose(matrix):\n",
    "    return [[row[i] for row in matrix] for i in range(len(matrix[0]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88306231-06b6-439c-85c2-1b4f7c0772ef",
   "metadata": {},
   "source": [
    "## 3.4. Matrix Multiplication\n",
    "**Matrix multiplication** is a way of **combining two matrices** to create a new one. This operation is essential in many areas of linear algebra, including solving systems of linear equations and applying transformations. It is important for multiple linear regression because it allows you to **calculate the coefficients of the regression model** by multiplying the inverse of the design matrix with the target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec4fcff-2109-422b-a0eb-5ed655f23716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Matrix Multiplication\n",
    "def matmul(A, B):\n",
    "    result = []\n",
    "    for i in range(len(A)):\n",
    "        row = []\n",
    "        for j in range(len(B[0])):\n",
    "            val = sum(A[i][k] * B[k][j] for k in range(len(B)))\n",
    "            row.append(val)\n",
    "        result.append(row)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac6552-3ce6-4ec8-8226-3ecacc1ca65e",
   "metadata": {},
   "source": [
    "## 3.5. Multiple Linear Regression\n",
    "With these mathematical basics, the **multiple linear regression** can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f97c72-588c-4468-b155-b2c4882b98d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    " # Multiple Linear Regression\n",
    "def multivariate_regression(X_raw, y):\n",
    "    X = [[1] + row for row in X_raw]\n",
    "    y_vec = [[val] for val in y]\n",
    "    \n",
    "    XT = transpose(X)\n",
    "    XTX = matmul(XT, X)\n",
    "    XTX_inv = invert_matrix(XTX)\n",
    "    XTy = matmul(XT, y_vec)\n",
    "    \n",
    "    beta = matmul(XTX_inv, XTy)\n",
    "    return [b[0] for b in beta]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a957264e-079d-4bd3-8e81-7c236c52b290",
   "metadata": {},
   "source": [
    "## 3.6. Predict Function\n",
    "A slightly modified **predict function** is required to **determine** the respective **y values** for the underlying **x values**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1893ed4-24b0-4dda-9350-74997464512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Function\n",
    "def predict_multi(X_raw, beta):\n",
    "    X = [[1] + row for row in X_raw]\n",
    "    return [sum(b * x for b, x in zip(beta, row)) for row in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32746a-8c22-4f9c-bc17-a860a25d7ae1",
   "metadata": {},
   "source": [
    "## 3.7. Residuals\n",
    "Again, **residuals** represent the **differences** between the observed values and the predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0435ccc-7193-4e08-bc93-4e7f6e6b05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals\n",
    "def residuals_multi(X_raw, y, beta):\n",
    "    y_pred = predict_multi(X_raw, beta)\n",
    "    return [yi - y_hat for yi, y_hat in zip(y, y_pred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce7130-c5c8-4b85-8bc0-6a9b70171653",
   "metadata": {},
   "source": [
    "## 3.8. Coefficient of Determination\n",
    "The **coefficient of determination** for a multiple linear regression model measures **how well the model's predictions match the actual data**. It indicates the proportion of the variance in the target variable that can be explained by the model.  It's interpretation is therefore similar to the coefficient of determination of a single linear regression model and may vary between 0 and 1, while a value closer to 0 means the model doesn't explain much of the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f839e-cf8a-4d4e-ab14-fdea0e1d6424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient of Determination\n",
    "def r_squared_multi(X_raw, y, beta):\n",
    "    y_pred = predict_multi(X_raw, beta)\n",
    "    y_mean = sum(y) / len(y)\n",
    "    \n",
    "    ss_tot = sum((yi - y_mean) ** 2 for yi in y)\n",
    "    ss_res = sum((yi - y_hat) ** 2 for yi, y_hat in zip(y, y_pred))\n",
    "    \n",
    "    return 1 - ss_res / ss_tot if ss_tot != 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f4eb2b-b2b2-4051-ab5a-838a44ddfbc1",
   "metadata": {},
   "source": [
    "## 3.9. Application\n",
    "Finally, these are **application examples** for the multiple linear regression coefficients with coresponding predictions for one case, the residuals of the model as well as the coefficient of determination in MicroPython:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9f5866-1df6-4520-857d-02772e98468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application Example\n",
    "beta = multivariate_regression(X, y)\n",
    "print(\"Coefficients:\")\n",
    "for i, b in enumerate(beta):\n",
    "    if i == 0:\n",
    "        print(\"a =\", b)\n",
    "    else:\n",
    "        print(\"b{} = {}\".format(i, b))\n",
    "\n",
    "x_case_13 = [X[12]]\n",
    "y_pred_13 = predict_multi(x_case_13, beta)[0]\n",
    "print(\"\\nPredictions for x1 = 11.4 and x2 = 76:\", y_pred_13)\n",
    "\n",
    "residuals = residuals_multi(X, y, beta)\n",
    "print(\"\\nResiduals:\", residuals)\n",
    "\n",
    "r2 = r_squared_multi(X, y, beta)\n",
    "print(\"\\nCoefficient of Determination:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5c822-e74d-4203-9702-c5c3ad840d2c",
   "metadata": {},
   "source": [
    "# 4. Machine Learning - II (Classification)\n",
    "![title](img/four.png)\n",
    "Another multivariate approach can be demonstrated via multiple logistic regression. This time, the dependent variable is nominally scaled and enables a **distinction to be made** between classes 0 or 1. As a result, this **multivariate statistics** approach can be used for **classifcation** tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45842089-f9ce-4c8c-8fdd-9277b4fbad40",
   "metadata": {},
   "source": [
    "## 4.1. Dataset\n",
    "Three variables of the **trees dataset**, provided by Atkinson, A. C. (1985): *Plots, Transformations and Regression* via Oxford University Press. The dependant variable has been dichotomized, whereby a volume greater than 20 results in 1, else 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2be138-cc96-4ed2-a667-fea18753c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girth (x1), Height (x2) and Binary Volume (y) of Black Cherry Trees\n",
    "X = [[8.3, 70], [8.6, 65], [8.8, 63], [10.5, 72], [10.7, 81], [10.8, 83], [11, 66], [11, 75], [11.1, 80], [11.2, 75],\n",
    "    [11.3, 79], [11.4, 76], [11.4, 76], [11.7, 69], [12, 75], [12.9, 74], [12.9, 85], [13.3, 86], [13.7, 71], [13.8, 64],\n",
    "    [14, 78], [14.2, 80], [14.5, 74], [16, 72], [16.3, 77], [17.3, 81], [17.5, 82], [17.9, 80], [18, 80], [18, 80], [20.6, 87]]\n",
    "\n",
    "\n",
    "y = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bcf888-fdd8-49ba-a48c-ac9e5c5ea9e1",
   "metadata": {},
   "source": [
    "## 4.2. Sigmoid Function\n",
    "The **sigmoid function** in (multiple) logistic regression maps any **input value to a range between 0 and 1**, allowing us to interpret the result as a probability by producing an S-shaped curve ideal for binary classification with 0=no and 1=yes, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31090e0b-f743-431c-901e-a94a43c353c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Sigmoid Function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + pow(2.71828, -z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9f9d4-d196-4158-ab04-d962feea5ec8",
   "metadata": {},
   "source": [
    "## 4.3. Log Function\n",
    "The **log function** approximates the **natural logarithm** using a numerical method based on the limit definition, useful when built-in log functions are unavailable in MicroPython. The natural logarithm (ln) is the **inverse of the exponential function** and tells us how many times we must multiply e≈2.71828 to get a given number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40867a5-b5fb-4806-821b-ab14ba743f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Log Function\n",
    "def log(x):\n",
    "    n = 1000.0\n",
    "    return n * ((x**(1/n)) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232ca42f-8e76-4c8a-9ecd-4b8a8015947d",
   "metadata": {},
   "source": [
    "## 4.4. Prediction of Probabilities\n",
    "As a result of these mathematical basics, a function for the **prediction of probabilities** is required for processing the values of the previous sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5504c-4bff-4a0e-b934-28f7c65a1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Prediction of Probability\n",
    "def predict_proba(x_input, weights, bias):\n",
    "    z = bias\n",
    "    for i in range(len(x_input)):\n",
    "        z += weights[i] * x_input[i]\n",
    "    return sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbeab7e-cd55-4337-9ade-89d56ea02a7b",
   "metadata": {},
   "source": [
    "## 4.5. Gradient Descent Training\n",
    "This function trains a logistic regression model using **gradient descent**. It iteratively **updates the weights and biases** to minimize the error between predicted probabilities (from the sigmoid function) and actual labels. By adjusting the weights in the direction that reduces the loss, the **model gradually learns** to classify input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba09a35-fb83-4789-adf9-b81642424a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate Logistic Regression via Gradient Descent\n",
    "def train_logistic_regression(X, y, lr=0.01, epochs=5000):\n",
    "    n_samples = len(X)\n",
    "    n_features = len(X[0])\n",
    "    weights = [0] * n_features\n",
    "    bias = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        grad_w = [0] * n_features\n",
    "        grad_b = 0\n",
    "        for i in range(n_samples):\n",
    "            z = bias\n",
    "            for j in range(n_features):\n",
    "                z += weights[j] * X[i][j]\n",
    "            p = sigmoid(z)\n",
    "            error = p - y[i]\n",
    "            for j in range(n_features):\n",
    "                grad_w[j] += error * X[i][j]\n",
    "            grad_b += error\n",
    "        for j in range(n_features):\n",
    "            weights[j] -= lr * grad_w[j] / n_samples\n",
    "        bias -= lr * grad_b / n_samples\n",
    "\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fe6cc5-0222-4d7d-8755-091b181239f3",
   "metadata": {},
   "source": [
    "## 4.6. Predict Function\n",
    "Again, a slightly modified **predict function** is required to determine the respective y values for the underlying x values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d0f35-1e82-4734-8e8d-b96820b46ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Prediction of Multivariate Logistic Regression\n",
    "def predict(x_input, weights, bias):\n",
    "    p = predict_proba(x_input, weights, bias)\n",
    "    return 1 if p >= 0.5 else 0, p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ccb60-abd3-47d3-8718-78b8c2d9e551",
   "metadata": {},
   "source": [
    "## 4.7. Application\n",
    "The **application examples** for the multiple logistic regression focus on weights and bias of the model and return **logits and probabilities as values for classification**. A classification example highlights the functionalty of multiple logistic regression models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c5c8d-6684-4a26-aa0e-1d092a2a9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application Examples\n",
    "weights, bias = train_logistic_regression(X, y)\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Intercept:\", bias)\n",
    "\n",
    "print(\"\\nLogits and Probabilities:\")\n",
    "for i in range(len(X)):\n",
    "    z = bias + sum([weights[j] * X[i][j] for j in range(len(weights))])\n",
    "    p = sigmoid(z)\n",
    "    print(\"x =\", X[i], \"Logit =\", z, \"P(y=1) =\", p)\n",
    "\n",
    "classification = predict([11.4, 76], weights, bias)\n",
    "print(\"\\nPredicted Class:\", classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be745d1-84c8-4610-916f-852a536aa5dc",
   "metadata": {},
   "source": [
    "# 5. Machine Learning - III (Clustering)\n",
    "![title](img/five.png)\n",
    "It is possible that not all cases in a data set are equivalent. Accordingly, **similar cases can be clustered** to enable detailed analyses of the corresponding clusters. Many different clustering algorithms are available and **k-Means clustering** will be demonstrated since it is particularly illustrative and commonly used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddbf068-dd7c-4955-b762-04bde0d71420",
   "metadata": {},
   "source": [
    "## 5.1. Dataset\n",
    "Two variables and 15 cases of the original **trees dataset**, provided by Atkinson, A. C. (1985): *Plots, Transformations and Regression* via Oxford University Press. The other 15 cases are simulated trees, based upon another type of tree. Therefore, the dependant variable is dichotomized, indicating black cherry trees from the original dataset by 0 and simulated trees by 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377f9c9-5fc0-47a6-a315-3aba5f69c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girth (x1), Height (x2) and Class (y) of Black Cherry Trees and Simulated Trees\n",
    "X = [\n",
    "    [8.3, 70], [8.6, 65], [8.8, 63], [10.5, 72], [10.7, 81], [10.8, 83], [11.0, 66], [11.0, 75], [11.1, 80],\n",
    "    [11.2, 75], [11.3, 79], [11.4, 76], [11.7, 69], [12.0, 75], [12.9, 74], [5.2, 45], [5.5, 48], [6.0, 50],\n",
    "    [6.3, 46], [6.7, 49], [7.0, 51], [7.2, 47], [7.4, 52], [7.5, 50], [7.7, 46], [7.9, 53], [8.1, 49],\n",
    "    [8.4, 47], [8.5, 54], [8.7, 52]\n",
    "]\n",
    "\n",
    "# y = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1 ,1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025152de-6ebb-4bd0-b2a4-9d2c2c74bd0e",
   "metadata": {},
   "source": [
    "## 5.2. Euclidean Distance\n",
    "The **euclidean distance** measures the **straight-line distance** between two points in a multi-dimensional space, calculated as the square root of the sum of the squared differences between corresponding coordinates. It’s commonly used in clustering and classification tasks to determine **similarity between data points**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0e8ae-3f0f-4e18-8173-83915a0c5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Euclidean Distance\n",
    "def euclidean_distance(p1, p2):\n",
    "    return sum((p1[i] - p2[i])**2 for i in range(len(p1))) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095a1a3-c5f8-4815-974a-71f4d13246c4",
   "metadata": {},
   "source": [
    "## 5.3. Centroids Function\n",
    "The **initializing centroids function** sets the **starting points for the cluster centers** and influences the convergence of the algorithm and the quality of the final clusters, as it determines **how the data is grouped** during the iterative process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1910808-8f03-420f-9570-1a110796da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Centroids Function\n",
    "def initialize_centroids(X, k):\n",
    "    return [X[i][:] for i in range(k)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068c2b9d-3088-4b0a-817b-337ac249fa53",
   "metadata": {},
   "source": [
    "## 5.4. Assigning Clusters Function\n",
    "The **assigning clusters function** groups data points into **clusters based on their proximity to the centroids**. For each point, it calculates the euclidean distance to each centroid and assigns the point to the closest centroid’s cluster, ensuring that each cluster contains the points nearest to its respective centroid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeaa203-e72b-4ed2-a077-d2725c17a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning Clusters Function\n",
    "def assign_clusters(X, centroids):\n",
    "    clusters = [[] for _ in centroids]\n",
    "    for point in X:\n",
    "        distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
    "        min_index = distances.index(min(distances))\n",
    "        clusters[min_index].append(point)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65757e-9a40-46e2-ba82-8aef7302cff9",
   "metadata": {},
   "source": [
    "## 5.5. Computing Centroids Function\n",
    "The **computing centroids function** calculates the new centroids by finding the **mean of all points within each cluster**. For each cluster, it averages the values of each feature across all points, updating the centroid to represent the center of that cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d36a37-1443-47d6-84b4-3524e5e114a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Centroids Function\n",
    "def compute_centroids(clusters):\n",
    "    new_centroids = []\n",
    "    for cluster in clusters:\n",
    "        if not cluster:\n",
    "            continue\n",
    "        n_features = len(cluster[0])\n",
    "        mean = [0] * n_features\n",
    "        for point in cluster:\n",
    "            for i in range(n_features):\n",
    "                mean[i] += point[i]\n",
    "        mean = [val / len(cluster) for val in mean]\n",
    "        new_centroids.append(mean)\n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ea304-224d-4673-a8fb-c1757cee4087",
   "metadata": {},
   "source": [
    "## 5.6. Within Cluster Sum of Squares\n",
    "The **within cluster sum of squares** is defined as the **total squared distance between each point and its assigned cluster centroid**. It measures the compactness of the clusters, with **smaller values indicating tighter clusters**. The code computes this by summing the squared differences for all points in each cluster, relative to the centroid of that cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2045070-1148-4e3d-a83d-c9b766bc815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within Cluster Sum of Squares\n",
    "def wcss(clusters, centroids):\n",
    "    total = 0\n",
    "    for i in range(len(clusters)):\n",
    "        for point in clusters[i]:\n",
    "            total += sum((point[j] - centroids[i][j])**2 for j in range(len(point)))\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f52ee-4c50-4e47-9908-17f55b1ce15e",
   "metadata": {},
   "source": [
    "## 5.7. k-Means Algorithm\n",
    "The **k-means algorithm** groups data points into **k clusters**. It iteratively **assigns points to the closest centroids**, recalculates the centroids, and computes the within cluster sum of squares until the centroids no longer change or the maximum number of iterations is reached, returning the final within cluster sum of squares value to assess the clustering quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bf01b7-8537-48a2-a027-8fc91ef7199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Algorithm\n",
    "def kmeans_wcss(X, k=2, max_iter=100):\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    for _ in range(max_iter):\n",
    "        clusters = assign_clusters(X, centroids)\n",
    "        new_centroids = compute_centroids(clusters)\n",
    "        if new_centroids == centroids:\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "\n",
    "    labels = [0] * len(X)\n",
    "    for cluster_index, cluster_points in enumerate(clusters):\n",
    "        for point in cluster_points:\n",
    "            for idx, original_point in enumerate(X):\n",
    "                if original_point == point:\n",
    "                    labels[idx] = cluster_index\n",
    "                    break\n",
    "\n",
    "    return wcss(clusters, centroids), labels, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12459e55-0c51-4d2d-abb6-cb7e9b781f29",
   "metadata": {},
   "source": [
    "## 5.8. k-Means Indicator\n",
    "The **k-means indicator** highlights the chance of the within cluster sum of squares values when the number of centroids is increased. A **decreasing value indicates a better allocation** of the cases to the centroids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab3927-cf15-4914-b899-43470bf6f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Indicator\n",
    "def kmeans_indicator(X, max_k=10):\n",
    "    wcss_values = []\n",
    "    for k in range(1, max_k + 1):\n",
    "        wcss_value, _, _ = kmeans_wcss(X, k)\n",
    "        wcss_values.append(wcss_value)\n",
    "    return wcss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce3bff1-870c-47a5-9494-4fa1aec3201a",
   "metadata": {},
   "source": [
    "## 5.9. Application\n",
    "The **application examples** indicate the within cluster sum of squares for each number of clusters. In addition, it indicates the number of clusters within the dataset, which in this case is supposed to be 2 and assigns the labels accordingly. The position of the centroids is highlighted as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbbc3bb-c6d0-4ce3-83ae-eaca0fd849b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Application examples\n",
    "wcss_list = kmeans_indicator(X, max_k=6)\n",
    "\n",
    "print(\"WCSS values for k = 1 to 6:\")\n",
    "for k, w in enumerate(wcss_list, 1):\n",
    "    print(\"k =\", k, \"-> WCSS =\", w)\n",
    "\n",
    "wcss_value, cluster_labels, final_centroids = kmeans_wcss(X, k=2)\n",
    "\n",
    "print(\"\\nWCSS:\", wcss_value)\n",
    "print(\"\\nCluster Labels:\", cluster_labels)\n",
    "print(\"\\nCentroids:\", final_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d27927-2434-4ca6-9bec-93ff6597d2ef",
   "metadata": {},
   "source": [
    "# 6. Machine Learning - IV (Dimensionality Reduction)\n",
    "![title](img/six.png)\n",
    "A **factor analysis** is a statistical method that reduces a large number of variables into a smaller, more manageable set of underlying factors. It helps identify **hidden patterns and relationships within data**, making it easier to understand complex structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc57607-4fb3-477d-95d4-1e02bada8c93",
   "metadata": {},
   "source": [
    "## 6.1. Dataset\n",
    "These are 10 variables, based upon 5 variables each for the two personality dimensions extraversion and neuroticism, from the **bfi dataset** by Revelle, W., Wilt, J. and A. Rosenthal (2010): *Individual Differences in Cognition: New Methods for examining the Personality-Cognition Link* via Springer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbddabab-f2b7-4f22-af4b-d9d4381d587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraversion (x1:x5) and Neuroticism (x6:x10) of the Big Five Inventory\n",
    "X = [\n",
    "    [2, 1, 6, 5, 6, 3, 5, 2, 2, 3],\n",
    "    [3, 6, 4, 2, 1, 6, 3, 2, 6, 4],\n",
    "    [1, 3, 2, 5, 4, 3, 3, 4, 2, 3],\n",
    "    [3, 4, 3, 6, 5, 2, 4, 2, 2, 3],\n",
    "    [2, 1, 2, 5, 2, 2, 2, 2, 2, 2],\n",
    "    [2, 2, 4, 6, 6, 4, 4, 4, 6, 6],\n",
    "    [3, 2, 5, 5, 6, 2, 3, 3, 1, 1],\n",
    "    [1, 1, 6, 6, 6, 2, 3, 1, 2, 1],\n",
    "    [2, 4, 4, 2, 6, 3, 3, 5, 3, 2],\n",
    "    [1, 2, 6, 5, 4, 1, 4, 2, 2, 5],\n",
    "    [1, 2, 6, 5, 5, 5, 4, 4, 3, 1],\n",
    "    [1, 2, 4, 5, 5, 3, 2, 4, 1, 2],\n",
    "    [6, 6, 2, 1, 1, 1, 2, 1, 3, 6],\n",
    "    [3, 4, 3, 2, 3, 5, 3, 4, 4, 3],\n",
    "    [6, 6, 3, 2, 2, 2, 2, 2, 4, 1],\n",
    "    [3, 4, 3, 3, 5, 5, 6, 5, 5, 4],\n",
    "    [3, 2, 3, 6, 5, 1, 2, 1, 2, 1],\n",
    "    [4, 3, 4, 4, 4, 2, 2, 3, 3, 3],\n",
    "    [3, 3, 2, 5, 4, 2, 3, 1, 3, 2],\n",
    "    [6, 4, 4, 4, 3, 2, 2, 3, 4, 5]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dae611-11a7-4742-9640-a930bf58ac94",
   "metadata": {},
   "source": [
    "## 6.2. Mean Center Function\n",
    "Mean centering via a **mean center function**, is often used in data preprocessing to make the **dataset more suitable for machine learning algorithms** by ensuring all features contribute equally to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b721b47-c843-4ef7-a9ee-ab7a43822adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Center Function\n",
    "def mean_center(X):\n",
    "    cols = len(X[0])\n",
    "    rows = len(X)\n",
    "    means = [sum(X[i][j] for i in range(rows)) / rows for j in range(cols)]\n",
    "    centered = [[X[i][j] - means[j] for j in range(cols)] for i in range(rows)]\n",
    "    return centered, means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe74ba5-5d78-408e-9114-0ae46358a662",
   "metadata": {},
   "source": [
    "## 6.3. Correlation Matrix\n",
    "Again, the **correlation matrix** quantifies the **strength and direction of the linear relationship** between two variables. This code can be used to correlate several variables and summarize the corresponding values in one matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79a7480-ba0a-4a0a-baed-c9b8336cbd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "def correlation_matrix(X):\n",
    "    rows = len(X)\n",
    "    cols = len(X[0])\n",
    "    corr = [[0]*cols for _ in range(cols)]\n",
    "\n",
    "    for i in range(cols):\n",
    "        for j in range(cols):\n",
    "            xi = [row[i] for row in X]\n",
    "            xj = [row[j] for row in X]\n",
    "            num = sum(xi[k] * xj[k] for k in range(rows))\n",
    "            denom_i = sum(xi[k]**2 for k in range(rows)) ** 0.5\n",
    "            denom_j = sum(xj[k]**2 for k in range(rows)) ** 0.5\n",
    "            corr[i][j] = num / (denom_i * denom_j)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f312c679-c3a6-4080-8a2e-1f886fbf3130",
   "metadata": {},
   "source": [
    "## 6.4. Power Iteration Function\n",
    "The **power iteration function** is an algorithm used to **compute the dominant eigenvalues and eigenvectors of a matrix**. The process involves iteratively applying matrix-vector multiplication to a random initial vector and normalizing it to avoid overflow or underflow, which **allows the vector to converge to the eigenvector** corresponding to the largest eigenvalue: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22cf945-5b74-43db-8441-cb8f02485093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power Iteration Function \n",
    "def power_iteration(A, num_vectors=2, iterations=100):\n",
    "    n = len(A)\n",
    "    eigenvectors = []\n",
    "    eigenvalues = []\n",
    "\n",
    "    for _ in range(num_vectors):\n",
    "        b = [1.0]*n\n",
    "        for _ in range(iterations):\n",
    "            # Multiply A * b\n",
    "            Ab = [sum(A[i][j] * b[j] for j in range(n)) for i in range(n)]\n",
    "            norm = sum(x**2 for x in Ab) ** 0.5\n",
    "            b = [x / norm for x in Ab]\n",
    "        # Rayleigh quotient for eigenvalue\n",
    "        Ab = [sum(A[i][j] * b[j] for j in range(n)) for i in range(n)]\n",
    "        eigval = sum(b[i] * Ab[i] for i in range(n))\n",
    "        eigenvalues.append(eigval)\n",
    "        eigenvectors.append(b)\n",
    "\n",
    "        # Deflation\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                A[i][j] -= eigval * b[i] * b[j]\n",
    "\n",
    "    return eigenvalues, eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f8a0e2-d94d-4e0e-bf19-03d4bb530b33",
   "metadata": {},
   "source": [
    "## 6.5. Factor Loadings Function\n",
    "The **factor loadings function** computes the factor loadings **based on the correlation matrix, eigenvalues, and eigenvectors**. In factor analysis, factor loadings represent the **relationships between observed variables** and the **underlying latent factors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521b949-7c15-43eb-a159-fec7fce4fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor Loadings Function\n",
    "def factor_loadings(corr_matrix, eigenvalues, eigenvectors):\n",
    "    loadings = []\n",
    "    for i in range(len(corr_matrix)):\n",
    "        row = []\n",
    "        for j in range(len(eigenvectors)):\n",
    "            loading = eigenvectors[j][i] * (eigenvalues[j] ** 0.5)\n",
    "            row.append(loading)\n",
    "        loadings.append(row)\n",
    "    return loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bbb18a-86da-494e-86e1-5d9288c44459",
   "metadata": {},
   "source": [
    "## 6.6. Application\n",
    "This **application example** shows how to compute the correlation matrix, the eigenvalues as well as the corresponding factor loadings for identifying the underlying factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e727248-f8be-4e51-b488-ab9bea3ac908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application examples\n",
    "X_centered, means = mean_center(X)\n",
    "R = correlation_matrix(X_centered)\n",
    "eigvals, eigvecs = power_iteration([row[:] for row in R], num_vectors=2)\n",
    "loadings = factor_loadings(R, eigvals, eigvecs)\n",
    "\n",
    "print(\"Correlation matrix:\")\n",
    "for row in R:\n",
    "    print([\"{0:.2f}\".format(x) for x in row])\n",
    "\n",
    "print(\"\\nEigenvalues:\")\n",
    "for i, val in enumerate(eigvals):\n",
    "    print(\"Factor\", i+1, \":\", round(val, 3))\n",
    "\n",
    "print(\"\\nFactor Loadings:\")\n",
    "for i, row in enumerate(loadings):\n",
    "    print(\"V\" + str(i+1), \":\", [\"{0:.2f}\".format(x) for x in row])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dedb26-81bf-4b2f-97e1-580eb8eab74f",
   "metadata": {},
   "source": [
    "# 7. Deep Learning - I\n",
    "![title](img/seven.png)\n",
    "A neural network consists of **neurons and layers** that process data via **activation functions**. Weights and biases are necessary in order to activate a neuron and to reach out for other neurons in another layer. The first neural network will use pretrained weights and biases straight out of TensorFlow in order to identify and process **non-linear patterns** within datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242bfcd7-c77b-4f26-8b17-8d0e8851897e",
   "metadata": {},
   "source": [
    "## 7.1. Dataset\n",
    "These are five variables from the **iris dataset** by Fisher, R. (1936): *The use of multiple measurements in taxonomic problems* via John Wiley & Sons. The four independent variables are based upon length and width of the sepal leaf (x1 and x2) as well as the petal leaf (x3 and x4). All indipendent variables are standardized. Additionaly, the dependent variable differs between versicolor (0) and virginica (1) as different species of iris flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca5210-b794-48a9-a696-e831b573acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized independent variables (X) and dichotomized dependent variable (y)\n",
    "X = [[ 0.81575475, -0.21746808, -0.12904165, -0.65303909],\n",
    "         [ 0.05761837,  1.59476592,  0.84485761,  1.71304456],\n",
    "         [ 0.96738203,  0.68864892, -0.00730424, -0.41643072],\n",
    "         [ 2.02877297,  0.38660992,  2.06223168,  1.00321947],\n",
    "         [ 1.42226386,  0.99068792,  1.33180724,  0.29339437],\n",
    "         [ 0.81575475,  0.99068792,  1.21006983,  1.4764362 ],\n",
    "         [-1.00377258,  0.38660992, -0.49425387, -0.41643072],\n",
    "         [ 0.05761837, -0.51950708, -0.00730424,  0.29339437],\n",
    "         [ 0.36087292,  0.38660992,  1.08833242,  1.23982783],\n",
    "         [ 0.66412748,  0.38660992,  0.35790798,  1.4764362 ],\n",
    "         [ 0.05761837,  0.08457092,  0.84485761,  0.29339437],\n",
    "         [-0.70051802, -0.51950708,  0.23617057,  0.53000274],\n",
    "         [ 0.20924564, -0.21746808,  0.84485761,  1.00321947],\n",
    "         [-0.24563619,  0.08457092, -0.25077906, -0.65303909],\n",
    "         [-2.06516352, -1.42562408, -1.95510276, -1.59947255],\n",
    "         [-1.15539985, -1.42562408, -1.34641572, -1.36286418],\n",
    "         [ 0.05761837, -1.12358508, -0.00730424, -0.41643072],\n",
    "         [ 0.20924564,  0.08457092, -0.73772869, -0.88964745],\n",
    "         [-0.39726347, -0.51950708,  0.23617057, -0.17982236],\n",
    "         [ 0.5125002 ,  0.08457092, -0.37251647, -0.88964745]]\n",
    "\n",
    "y = [0,1,0,1,1,1,0,1,1,1,1,1,1,0,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3113d1-99fd-43b9-b873-b3b9e810f661",
   "metadata": {},
   "source": [
    "## 7.2. Libraries\n",
    "Normally all functions in MicroPython can be coded manually. However, the **math library** is imported here to simplify the execution the **exponential function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e099137-6561-4b64-9540-45d18764b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bbbd7f-a922-4119-9fed-437f9af660ee",
   "metadata": {},
   "source": [
    "## 7.3. Activation Functions\n",
    "Neural networks are based upon neurons and **activation functions** decide whether a neuron should be activated or not. This means that it will decide **whether the neuron's input to the neural network is important or not** in the process of prediction using simpler mathematical operations like Rectified Linear Unit (ReLU), Leaky Rectified Linear Unit (Leaky ReLU), Hyperbolig Tangent (Tanh) or Logistic Regression (Sigmoid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92f97ac-0b26-4d99-82b6-f8194d4f9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU\n",
    "def relu(x):\n",
    "    return [max(0, val) for val in x]\n",
    "\n",
    "# Leaky ReLU\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return [val if val >= 0 else alpha * val for val in x]\n",
    "\n",
    "# Tanh\n",
    "def tanh(x):\n",
    "    return [(math.exp(val) - math.exp(-val)) / (math.exp(val) + math.exp(-val)) for val in x]\n",
    "\n",
    "# Sigmoid\n",
    "def sigmoid(x):\n",
    "    return [1 / (1 + math.exp(-val)) for val in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fc163a-c1b2-47bc-b201-7e6c3da7ce41",
   "metadata": {},
   "source": [
    "## 7.4. Single Neuron\n",
    "A **single neuron** therefore accesses one of the **previously defined activation functions** and can be defined as follows in MicroPython:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c9d6c-79d7-4a60-9054-a5c2cba90040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Neuron\n",
    "def neuron(x, w, b, activation):\n",
    "\n",
    "    tmp = zero_dim(x[0])\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        tmp = add_dim(tmp, [(float(w[i]) * float(x[i][j])) for j in range(len(x[0]))])\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        yp = sigmoid([tmp[i] + b for i in range(len(tmp))])\n",
    "    elif activation == \"relu\":\n",
    "        yp = relu([tmp[i] + b for i in range(len(tmp))])\n",
    "    elif activation == \"leaky_relu\":\n",
    "        yp = relu([tmp[i] + b for i in range(len(tmp))])\n",
    "    elif activation == \"tanh\":\n",
    "        yp = tanh([tmp[i] + b for i in range(len(tmp))])\n",
    "    else:\n",
    "        print(\"Function unknown!\")\n",
    "\n",
    "    return yp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60425596-dec2-4728-9aeb-1f4921f13821",
   "metadata": {},
   "source": [
    "## 7.5. Data Formats and Processing\n",
    "In order for the **data to be adequately processed** by a neural network, a series of **data formats** such as **vectors, matrices and the architecture of neural networks via layers** must be defined. These **mathematical basics** of a neural network can be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697d0ba-eadd-4d8c-97b8-58c47382945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - I\n",
    "def zero_dim(x):\n",
    "    z = [0 for i in range(len(x))]\n",
    "    return z\n",
    "\n",
    "# Mathematical Basics - II\n",
    "def add_dim(x, y):\n",
    "    z = [x[i] + y[i] for i in range(len(x))]\n",
    "    return z\n",
    "\n",
    "# Mathematical Basics - III\n",
    "def zeros(rows, cols):\n",
    "    M = []\n",
    "    while len(M) < rows:\n",
    "        M.append([])\n",
    "        while len(M[-1]) < cols:\n",
    "            M[-1].append(0.0)\n",
    "    return M\n",
    "\n",
    "# Mathematical Basics - IV\n",
    "def transpose(M):\n",
    "    if not isinstance(M[0], list):\n",
    "        M = [M]\n",
    "    rows = len(M)\n",
    "    cols = len(M[0])\n",
    "    MT = zeros(cols, rows)\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            MT[j][i] = M[i][j]\n",
    "    return MT\n",
    "\n",
    "# Mathematical Basics - V\n",
    "def print_matrix(M, decimals=3):\n",
    "    for row in M:\n",
    "        print([round(x, decimals) + 0 for x in row])\n",
    "\n",
    "# Mathematical Basics - VI\n",
    "def dense(nunit, x, w, b, activation):\n",
    "    res = []\n",
    "    for i in range(nunit):\n",
    "        z = neuron(x, w[i], b[i], activation)\n",
    "        res.append(z)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42692483-6bc9-457a-ac0a-83ed25f8739f",
   "metadata": {},
   "source": [
    "## 7.6. Weights and Biases\n",
    "The **architecture of a neural network** can be reconstructed in MicroPython with the **weights and biases** from a already pretrained deep learning model. They can be **transferred from TensorFlow** (which is a deep learning library suitable for Python) to MicroPython. The following structure indicates four independent variables (rows) for two neurons (columns) in the input layer with the according weight w1. In addition, the first layer has two accoring biases b1. Therefore, the first hidden layer consists of three neurons with w2 and b2, the second hidden layer consists of two neurons with w3 and b3 and the output layer is a single neuron with w4 and b4. As a result, this neural network consists of a total of eight neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e132c98-e540-4276-8285-f039bf2f6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include Parameters from TensorFlow\n",
    "w1 = [[-0.75323504, -0.25906014],\n",
    "      [-0.46379513, -0.5019245 ],\n",
    "      [ 2.1273055 ,  1.7724446 ],\n",
    "      [ 1.1853403 ,  0.88468695]]\n",
    "b1 = [0.53405946, 0.32578036]\n",
    "w2 = [[-1.6785783,  2.0158117,  1.2769054],\n",
    "      [-1.4055765,  0.6828738,  1.5902631]]\n",
    "b2 = [ 1.18362  , -1.1555661, -1.0966455]\n",
    "w3 = [[ 0.729278  , -1.0240695 ],\n",
    "      [-0.80972326,  1.4383037 ],\n",
    "      [-0.90892404,  1.6760625 ]]\n",
    "b3 = [0.10695826, 0.01635581]\n",
    "w4 = [[-0.2019448],\n",
    "      [ 1.5772797]]\n",
    "b4 = [-1.2177287]\n",
    "\n",
    "# Transpose\n",
    "w1 = transpose(w1)\n",
    "w2 = transpose(w2)\n",
    "w3 = transpose(w3)\n",
    "w4 = transpose(w4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a71df6-88bd-479f-9ac8-f6a7c4626c33",
   "metadata": {},
   "source": [
    "## 7.7. Neural Network Architecture\n",
    "According to the transferred weights and biases the **architecture of the neural network** can be defined in MicroPython as follows. This specifies the **number of neurons within each layer** and the **activation functions** for activating the neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2173fd61-3840-40ab-a11f-bece19bfedb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Architecture\n",
    "yout1 = dense(2, transpose(X), w1, b1, 'relu') # input layer (2 neurons)\n",
    "yout2 = dense(3, yout1, w2, b2, 'sigmoid') # hidden layer (3 neurons)\n",
    "yout3 = dense(2, yout2, w3, b3, 'relu') # hidden layer (2 neurons)\n",
    "ypred = dense(1, yout3, w4, b4,'sigmoid') # output layer (1 neuron)\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38bc20-35f6-43fd-ba9e-ad19e09ad9f9",
   "metadata": {},
   "source": [
    "## 7.8. Confusion Matrix\n",
    "A **confusion matrix**, also known as an error matrix, is a table that **visualizes the performance of a classification model** by comparing its predictions against the actual results. It's a two-dimensional matrix that displays the counts of true positives, true negatives, false positives, and false negatives, providing a detailed view of **where a model's predictions are correct and where it's making errors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485790f0-ecbe-4287-97d3-dc26d1883f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Basics\n",
    "def classification_report(y, ypred):\n",
    "    TP = TN = FP = FN = 0\n",
    "    for true, pred in zip(y, ypred):\n",
    "        if true == pred:\n",
    "            if true == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "        else:\n",
    "            if true == 1:\n",
    "                FN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    accuracy = (TP + TN) / len(y)\n",
    "    print(\"Accuracy: {:.3f}\".format(accuracy))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"TN: {}, FP: {}\".format(TN, FP))\n",
    "    print(\"FN: {}, TP: {}\".format(FN, TP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2863a37-4578-4865-820d-9c2bf481690b",
   "metadata": {},
   "source": [
    "## 7.9. Application\n",
    "The **performance of the pretrained neural network** can be viewed via the following MicroPython code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff625e-3e57-4fdb-924b-7cc7024deb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "ypred_class = [1 if i > 0.5 else 0 for i in ypred[0]]\n",
    "print(ypred_class)\n",
    "print(classification_report(y, ypred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e03b2f-b797-40bb-a800-bacffeeacfd7",
   "metadata": {},
   "source": [
    "# 8. Deep Learning - II\n",
    "![title](img/seven.png)\n",
    "The second neural network will adjust weights and biases automatically. As a result, this neural network can identify and process **non-linear patterns** within datasets on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbac749-739f-419e-8e7b-a2eec8b8625f",
   "metadata": {},
   "source": [
    "## 8.1. Dataset\n",
    "Again, the five variables from the **iris dataset** by Fisher, R. (1936): *The use of multiple measurements in taxonomic problems* via John Wiley & Sons will be used. The four independent variables are based upon length and width of the sepal leaf (x1 and x2) as well as the petal leaf (x3 and x4). All indipendent variables are standardized. Additionaly, the dependent variable differs between versicolor (0) and virginica (1) as different species of iris flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b269792-bcad-46f9-a3a3-656fa0a09a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized independent variables (X) and dichotomized dependent variable (y)\n",
    "X = [[ 0.81575475, -0.21746808, -0.12904165, -0.65303909],\n",
    "         [ 0.05761837,  1.59476592,  0.84485761,  1.71304456],\n",
    "         [ 0.96738203,  0.68864892, -0.00730424, -0.41643072],\n",
    "         [ 2.02877297,  0.38660992,  2.06223168,  1.00321947],\n",
    "         [ 1.42226386,  0.99068792,  1.33180724,  0.29339437],\n",
    "         [ 0.81575475,  0.99068792,  1.21006983,  1.4764362 ],\n",
    "         [-1.00377258,  0.38660992, -0.49425387, -0.41643072],\n",
    "         [ 0.05761837, -0.51950708, -0.00730424,  0.29339437],\n",
    "         [ 0.36087292,  0.38660992,  1.08833242,  1.23982783],\n",
    "         [ 0.66412748,  0.38660992,  0.35790798,  1.4764362 ],\n",
    "         [ 0.05761837,  0.08457092,  0.84485761,  0.29339437],\n",
    "         [-0.70051802, -0.51950708,  0.23617057,  0.53000274],\n",
    "         [ 0.20924564, -0.21746808,  0.84485761,  1.00321947],\n",
    "         [-0.24563619,  0.08457092, -0.25077906, -0.65303909],\n",
    "         [-2.06516352, -1.42562408, -1.95510276, -1.59947255],\n",
    "         [-1.15539985, -1.42562408, -1.34641572, -1.36286418],\n",
    "         [ 0.05761837, -1.12358508, -0.00730424, -0.41643072],\n",
    "         [ 0.20924564,  0.08457092, -0.73772869, -0.88964745],\n",
    "         [-0.39726347, -0.51950708,  0.23617057, -0.17982236],\n",
    "         [ 0.5125002 ,  0.08457092, -0.37251647, -0.88964745]]\n",
    "\n",
    "y = [0,1,0,1,1,1,0,1,1,1,1,1,1,0,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b22af1-94e2-44e7-b60c-9cee236941ed",
   "metadata": {},
   "source": [
    "## 8.2. Libraries\n",
    "The **random library** and **math library** are imported to simplify the **execution of some functions** required for self-learning neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b13d33a-eedd-4fda-868f-c4b972d8b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc1b594-dc1b-4ad6-a907-50eca27f6330",
   "metadata": {},
   "source": [
    "## 8.3. Activation Functions and Derivatives\n",
    "Self-learning neural networks not only require **activation functions**, but also their derivates. The **derivative of a function** represents its instantaneous rate of change at a specific point. This allows the neural network to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a69231f-5ec6-4bc3-b3b0-7c4543f3bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# Derivate of Sigmoid\n",
    "def sigmoid_derivative(output):\n",
    "    return output * (1 - output)\n",
    "\n",
    "# ReLU\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "# Derivate of ReLU\n",
    "def relu_derivative(output):\n",
    "    return 1 if output > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a292fc-aaf2-4494-b94d-150613969bb4",
   "metadata": {},
   "source": [
    "## 8.4. Function for Random Initialization\n",
    "Since the neural network is supposed to learn the weights and biases by itself, the layers and neurons of the neural network will be **initialized with some random values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249982aa-8ef6-48d8-98e6-0df97bf3ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Initializing Weights and Biases\n",
    "def init_layer(input_size, output_size):\n",
    "    weights = [[random.uniform(-0.5, 0.5) for _ in range(input_size)] for _ in range(output_size)]\n",
    "    biases = [random.uniform(-0.5, 0.5) for _ in range(output_size)]\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ff0879-314e-4169-811f-a1f2ac51cd29",
   "metadata": {},
   "source": [
    "## 8.5. Forward and Backward Data Processing\n",
    "In neural networks, **forward propagation** is the process of passing input data through the network's layers to generate a prediction and **backward propagation**, on the other hand, is the mechanism used to train the network by calculating the error between the prediction and the actual output, and then adjusting the network's weights to minimize that error. This important for the learning ability of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3945ce4-cc5b-4a69-91c8-95894225536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Propagation\n",
    "def dense_forward(inputs, weights, biases, activation='relu'):\n",
    "    outputs = []\n",
    "    pre_activations = []\n",
    "    for w, b in zip(weights, biases):\n",
    "        z = sum(i*w_ij for i, w_ij in zip(inputs, w)) + b\n",
    "        pre_activations.append(z)\n",
    "        if activation == 'sigmoid':\n",
    "            outputs.append(sigmoid(z))\n",
    "        elif activation == 'relu':\n",
    "            outputs.append(relu(z))\n",
    "        else:\n",
    "            raise Exception(\"Unknown activation\")\n",
    "    return outputs, pre_activations\n",
    "\n",
    "# Backward Propagation\n",
    "def dense_backward(inputs, grad_outputs, outputs, pre_activations, weights, biases, activation='relu', lr=0.01):\n",
    "    input_grads = [0.0 for _ in range(len(inputs))]\n",
    "    for j in range(len(weights)):\n",
    "        if activation == 'sigmoid':\n",
    "            delta = grad_outputs[j] * sigmoid_derivative(outputs[j])\n",
    "        elif activation == 'relu':\n",
    "            delta = grad_outputs[j] * relu_derivative(pre_activations[j])\n",
    "        else:\n",
    "            raise Exception(\"Unknown activation\")\n",
    "        for i in range(len(inputs)):\n",
    "            input_grads[i] += weights[j][i] * delta\n",
    "            weights[j][i] -= lr * delta * inputs[i]\n",
    "        biases[j] -= lr * delta\n",
    "    return input_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe56a4-2ba9-48bc-9217-6e6cf96f3e92",
   "metadata": {},
   "source": [
    "## 8.6. Loss Function\n",
    "Furthermore, a **loss function** quantifies the **difference between a deep learning model's prediction and the actual outcome**, essentially acting as a measure of the model's error. Cross-entropy, a specific type of loss function, is commonly used for classification problems, especially when the model outputs probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007916f2-445d-4676-a9c8-1bfe1a18791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "def binary_cross_entropy(predicted, target):\n",
    "    epsilon = 1e-7\n",
    "    return - (target * math.log(predicted + epsilon) + (1 - target) * math.log(1 - predicted + epsilon))\n",
    "\n",
    "def binary_cross_entropy_derivative(predicted, target):\n",
    "    epsilon = 1e-7\n",
    "    return -(target / (predicted + epsilon)) + (1 - target) / (1 - predicted + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e1f48-8157-4260-ae43-c4d8cb05fc8d",
   "metadata": {},
   "source": [
    "## 8.7. Neural Network Architecture\n",
    "This time the **architecture of the neural network** consists of four independent variables which will be forwarded to three neurons in the input layer and one neuron in the output layer. This is a very simple neural network that consists of four neurons in two layers with according weights (w1 and w2) and biases (b1 and b2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10476c1-ab4a-4e1e-b447-371c140fcf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights and Biases\n",
    "w1, b1 = init_layer(4, 3)\n",
    "w2, b2 = init_layer(3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b4e87-26f5-44f4-a4a4-5d410e7f9b8b",
   "metadata": {},
   "source": [
    "## 8.8. Specification of Learning Behavior\n",
    "Finally, the number of **epochs and the learning rate** need to be specified in MicroPython. In neural networks, an **epoch represents one complete pass of the entire training dataset** through the model. Learning rate determines **how much the model's weights are adjusted** during each update step in the training process. Both are crucial hyperparameters that influence training and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc95cf-de09-46ea-9ca1-d43951b2dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs and Learning Rate for Training\n",
    "epochs = 100\n",
    "lr = 0.05\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for xi, yi in zip(X, y):\n",
    "        # Forward pass\n",
    "        out1, pre1 = dense_forward(xi, w1, b1, 'relu')\n",
    "        out2, pre2 = dense_forward(out1, w2, b2, 'sigmoid')\n",
    "        loss = binary_cross_entropy(out2[0], yi)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Backward pass\n",
    "        dL_dout2 = [binary_cross_entropy_derivative(out2[0], yi)]\n",
    "        dL_dout1 = dense_backward(out1, dL_dout2, out2, pre2, w2, b2, 'sigmoid', lr)\n",
    "        _ = dense_backward(xi, dL_dout1, out1, pre1, w1, b1, 'relu', lr)\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ce189-fcc6-4b34-873e-0b28505cdc4c",
   "metadata": {},
   "source": [
    "## 8.9. Predict Funtion\n",
    "The **outcome of the neural network** can be predicted with the collowing code in MicroPython:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5afa99e-1986-49e0-9cd2-409ecf616865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Function\n",
    "def predict(x):\n",
    "    out1, _ = dense_forward(x, w1, b1, 'relu')\n",
    "    out2, _ = dense_forward(out1, w2, b2, 'sigmoid')\n",
    "    return 1 if out2[0] > 0.5 else 0\n",
    "\n",
    "ypred = [predict(xi) for xi in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c059ff06-98ec-4042-a97f-09581296e10b",
   "metadata": {},
   "source": [
    "## 8.10. Confusion Matrix\n",
    "As in the pretrained neural network before, a **confusion matrix** can be used to evaluate the **performance of the neural network**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da4a33-48e5-476d-b37a-a5e6e48ec602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Basics\n",
    "def classification_report(ytrue, ypred):\n",
    "    TP = TN = FP = FN = 0\n",
    "    for true, pred in zip(ytrue, ypred):\n",
    "        if true == pred:\n",
    "            if true == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "        else:\n",
    "            if true == 1:\n",
    "                FN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    accuracy = (TP + TN) / len(ytrue)\n",
    "    print(\"Accuracy: {:.3f}\".format(accuracy))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"TN: {}, FP: {}\".format(TN, FP))\n",
    "    print(\"FN: {}, TP: {}\".format(FN, TP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c0404-d36f-43b4-8f5c-594a45fe6fa0",
   "metadata": {},
   "source": [
    "## 8.11. Application\n",
    "Finally, the **performance of the neural network** can be inspected and validated with new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32279623-c72c-4990-86b3-dbba92d9851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "ypred = [predict(xi) for xi in X]\n",
    "\n",
    "# Show classification metrics\n",
    "classification_report(y, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b025d0c0-555d-40a1-8146-0a55bf7b7847",
   "metadata": {},
   "source": [
    "![title](img/the_end.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
