{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc2b15b0-cfbb-4413-a54e-ea07f37328a9",
   "metadata": {},
   "source": [
    "# Explainable Artificial Intelligence with MicroPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be470f7-97f5-410b-8d3f-216aadf85cf8",
   "metadata": {},
   "source": [
    "## Statistical Basics - I\n",
    "### Mean, Variance and Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff51e3-2da4-4bcc-bdaa-a6f7902f24ba",
   "metadata": {},
   "source": [
    "One variable of the **trees dataset**, provided by Atkinson, A. C. (1985): *Plots, Transformations and Regression* via Oxford University Press:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b29baa-231c-4524-87e1-81bf021c5ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girth (x) of Black Cherry Trees\n",
    "x = [8.3, 8.6, 8.8, 10.5, 10.7, 10.8, 11, 11, 11.1, 11.2, 11.3, \n",
    "     11.4, 11.4, 11.7, 12, 12.9, 12.9, 13.3, 13.7, 13.8, 14, \n",
    "     14.2, 14.5, 16, 16.3, 17.3, 17.5, 17.9, 18, 18, 20.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac4456c-dff9-4870-900c-5b80bd1e2a31",
   "metadata": {},
   "source": [
    "The **mean**, also known as the arithmetic mean, is one of the most common measures of central tendency in statistics. It represents the average value of a dataset and provides a single value that summarizes the entire data distribution. To calculate the mean, you sum all values in your dataset and divide this total by the number of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa1ddf-153d-4e35-8771-84cba0094232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "def mean(data):\n",
    "    return sum(data) / len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a79a9f-1a83-4e53-b343-7c703d0a2a8a",
   "metadata": {},
   "source": [
    "The **sample variance** is a measure of how spread out the values in a dataset are. It quantifies the average squared deviation from the mean, giving insight into the variability within the sample. Unlike population variance, it divides by n−1n−1 to account for the degrees of freedom, making it an unbiased estimator when working with a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc871ff2-d1ca-4f37-af29-cf17162483e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance\n",
    "def variance(data):\n",
    "    m = mean(data)\n",
    "    return sum((x - m) ** 2 for x in data) / (len(data) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43581b1-8b0d-4af6-af57-76e635fad585",
   "metadata": {},
   "source": [
    "The **standard deviation** is the square root of the variance and provides a measure of spread in the same units as the original data. It indicates how much the values in a dataset typically deviate from the mean, making it easier to interpret than variance in practical terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6840eb-173c-476a-a44a-3399da095331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Deviation\n",
    "def std_dev(data):\n",
    "    return variance(data) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf403c-e788-4fab-9402-a7f57d0c52c1",
   "metadata": {},
   "source": [
    "These are **application examples** for mean, sample variance and standard deviation in MicroPython:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8751d41-526d-422d-b8b6-0e661d195bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application Examples\n",
    "print(\"Mean\", mean(x))\n",
    "print(\"Variance\", variance(x))\n",
    "print(\"Standard Deviation\", std_dev(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e60abe2-36a6-4701-a2a0-0949bb6f4fe2",
   "metadata": {},
   "source": [
    "## Statistical Basics - II\n",
    "### Covariance, Correlation and Single Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ae0cf-1dd9-4f0c-a116-aef473462e82",
   "metadata": {},
   "source": [
    "Two variables of the **trees dataset**, provided by Atkinson, A. C. (1985): *Plots, Transformations and Regression* via Oxford University Press:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c060aa37-2c26-41ad-ba66-f32571bf99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girth (x) and Volume (y) of Black Cherry Trees\n",
    "x = [8.3, 8.6, 8.8, 10.5, 10.7, 10.8, 11, 11, 11.1, 11.2, 11.3, \n",
    "     11.4, 11.4, 11.7, 12, 12.9, 12.9, 13.3, 13.7, 13.8, 14, \n",
    "     14.2, 14.5, 16, 16.3, 17.3, 17.5, 17.9, 18, 18, 20.6]\n",
    "\n",
    "y = [10.3, 10.3, 10.2, 16.4, 18.8, 19.7, 15.6, 18.2, 22.6, 19.9,\n",
    "     24.2, 21, 21.4, 21.3, 19.1, 22.2, 33.8, 27.4, 25.7, 24.9,\n",
    "     34.5, 31.7, 36.3, 38.3, 42.6, 55.4, 55.7, 58.3, 51.5, 51, 77]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6c3e0-0b50-4f89-b7d5-40a1ae73a2d5",
   "metadata": {},
   "source": [
    "The **covariance** measures the directional relationship between two variables. A positive covariance indicates that the variables tend to increase together, while a negative covariance suggests that as one increases, the other tends to decrease. It's a foundational concept in statistics for understanding how two variables vary together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c002c-fdcf-4f53-b93c-c1b029e1f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance\n",
    "def covariance(x, y):\n",
    "    mx = mean(x)\n",
    "    my = mean(y)\n",
    "    return sum((x[i] - mx) * (y[i] - my) for i in range(len(x))) / (len(x) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25314fc-07eb-4d07-b91f-082a98d6f6c5",
   "metadata": {},
   "source": [
    "The **correlation** quantifies the strength and direction of the linear relationship between two variables. It standardizes the covariance by dividing it by the product of the standard deviations, resulting in a value between -1 and 1. A correlation close to 1 or -1 indicates a strong relationship, while a value near 0 suggests little to no linear association:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed232c0a-8090-41e9-9535-474fdb9460de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation\n",
    "def correlation(x, y):\n",
    "    return covariance(x, y) / (std_dev(x) * std_dev(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b311e-8071-4cd7-9911-0b5270333c03",
   "metadata": {},
   "source": [
    "A **simple linear regression** models the relationship between two variables by fitting a straight line to the data. It calculates the slope b and intercept a of the line y=a+bx, where b indicates how much y changes for each unit increase in x, and a is the predicted value of y when x=0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a5cb22-a7e8-4c5f-9c35-072c089f9bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression\n",
    "def linear_regression(x, y):\n",
    "    b = covariance(x, y) / variance(x)\n",
    "    a = mean(y) - b * mean(x)\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1869c6f-9bb5-42cd-99ee-e8dc58793030",
   "metadata": {},
   "source": [
    "The **predict function** is required to determine the respective y values for the underlying x values via a and b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5932f8-4db3-49e6-87d8-9f21391c09aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Function\n",
    "def predict(x_new, a, b):\n",
    "    return a + b * x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8feef56-f685-41e5-acb4-6d4541cc0af6",
   "metadata": {},
   "source": [
    "**Residuals** represent the differences between the observed values and the predicted values from a linear regression model. They indicate how well the model fits the data: a residual close to 0 means a good fit, while larger residuals suggest that the model doesn't capture the data as accurately. The residuals can be used to assess the assumptions of linear regression and identify any outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05fb58-7484-47bd-aff5-4edff4e75761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals\n",
    "def residuals(x, y, a, b):\n",
    "    return [y[i] - (a + b * x[i]) for i in range(len(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832348e8-a7ce-4c05-b99b-e4164dfda8ec",
   "metadata": {},
   "source": [
    "The **coefficient of determination** measures the proportion of variance in the dependent variable that is explained by the independent variable in a regression model. It indicates the goodness of fit: an coefficient of determination close to 1 means that the model explains most of the variance, while a value near 0 suggests the model doesn’t capture much of the variability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac66da-0f53-4b7b-9c71-cdaccd777386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient of Determination\n",
    "def r_squared(x, y, a, b):\n",
    "    y_mean = mean(y)\n",
    "    ss_tot = sum((yi - y_mean) ** 2 for yi in y)\n",
    "    ss_res = sum((y[i] - (a + b * x[i])) ** 2 for i in range(len(y)))\n",
    "    return 1 - ss_res / ss_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acdbc67-fbec-4df0-8849-6ee1d595c772",
   "metadata": {},
   "source": [
    "These are **application examples** for covariance, correlation, as well as the single linear regression with the coresponding predictions, residuals and the coefficient of determination in MicroPython:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4006c98-62e9-4ba2-a6b4-b7954d04c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apllication Examples\n",
    "print(\"Covariance:\", covariance(x, y))\n",
    "print(\"Correlation:\", correlation(x, y))\n",
    "\n",
    "a, b = linear_regression(x, y)\n",
    "print(\"\\nSingle Linear Regression: y = {:.2f} + {:.2f} * x\".format(a, b))\n",
    "print(\"Predictions for x = 11.4:\", predict(11.4, a, b))\n",
    "print(\"\\nResiduals:\", residuals(x, y, a, b))\n",
    "print(\"\\nCoefficient of Determination:\", r_squared(x, y, a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec27fa0-b62e-4b45-89a9-4e60d41bd58d",
   "metadata": {},
   "source": [
    "## Machine Learning: Regression\n",
    "### Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bddd3c7-1fa2-44e6-83e7-37af516de606",
   "metadata": {},
   "source": [
    "Three variables of the **trees dataset**, provided by Atkinson, A. C. (1985): *Plots, Transformations and Regression* via Oxford University Press:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b1a9d-be90-4639-9fe2-1c8fb20915a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girth (x1), Height (x2) and Volume (y) of Black Cherry Trees \n",
    "X = [[8.3, 70], [8.6, 65], [8.8, 63], [10.5, 72], [10.7, 81], [10.8, 83], [11, 66], [11, 75], [11.1, 80], [11.2, 75],\n",
    "    [11.3, 79], [11.4, 76], [11.4, 76], [11.7, 69], [12, 75], [12.9, 74], [12.9, 85], [13.3, 86], [13.7, 71], [13.8, 64],\n",
    "    [14, 78], [14.2, 80], [14.5, 74], [16, 72], [16.3, 77], [17.3, 81], [17.5, 82], [17.9, 80], [18, 80], [18, 80], [20.6, 87]]\n",
    "\n",
    "y = [10.3, 10.3, 10.2, 16.4, 18.8, 19.7, 15.6, 18.2, 22.6, 19.9,\n",
    "     24.2, 21, 21.4, 21.3, 19.1, 22.2, 33.8, 27.4, 25.7, 24.9,\n",
    "     34.5, 31.7, 36.3, 38.3, 42.6, 55.4, 55.7, 58.3, 51.5, 51, 77]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ea49aa-3b27-401f-9e16-6e429235a431",
   "metadata": {},
   "source": [
    "**Matrix inversion** is essential in solving systems of linear equations, particularly in methods like multiple linear regression. The following code implements the Gaussian elimination method to invert a matrix, ensuring it is invertible by checking for non-zero pivots during the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee2fb4-0a5d-4133-bcef-a461c8bcface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Matrix Inversion\n",
    "def invert_matrix(matrix):\n",
    "    n = len(matrix)\n",
    "    identity = [[float(i == j) for j in range(n)] for i in range(n)]\n",
    "    m = [row[:] for row in matrix]\n",
    "\n",
    "    for i in range(n):\n",
    "        max_row = i\n",
    "        max_val = abs(m[i][i])\n",
    "        for k in range(i + 1, n):\n",
    "            if abs(m[k][i]) > max_val:\n",
    "                max_val = abs(m[k][i])\n",
    "                max_row = k\n",
    "\n",
    "        if max_val == 0:\n",
    "            raise ValueError(\"Matrix is not invertible!\")\n",
    "\n",
    "        if max_row != i:\n",
    "            m[i], m[max_row] = m[max_row], m[i]\n",
    "            identity[i], identity[max_row] = identity[max_row], identity[i]\n",
    "\n",
    "        factor = m[i][i]\n",
    "        for j in range(n):\n",
    "            m[i][j] /= factor\n",
    "            identity[i][j] /= factor\n",
    "\n",
    "        for k in range(n):\n",
    "            if k != i:\n",
    "                factor = m[k][i]\n",
    "                for j in range(n):\n",
    "                    m[k][j] -= factor * m[i][j]\n",
    "                    identity[k][j] -= factor * identity[i][j]\n",
    "\n",
    "    return identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b855e19-bdb2-49d5-a101-c3e11386d9e6",
   "metadata": {},
   "source": [
    "**Matrix transposition** involves flipping a matrix over its diagonal, converting rows into columns and vice versa. The resulting matrix is called the transpose of the original matrix. Transposition is commonly used in linear algebra, especially in operations like solving systems of equations or adjusting data representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0959f-336b-46e7-a90c-d8696bf7d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Matrix Transposition\n",
    "def transpose(matrix):\n",
    "    return [[row[i] for row in matrix] for i in range(len(matrix[0]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88306231-06b6-439c-85c2-1b4f7c0772ef",
   "metadata": {},
   "source": [
    "**Matrix multiplication** is a way of combining two matrices to create a new one. This operation is essential in many areas of linear algebra, including solving systems of linear equations and applying transformations. It is important for multiple linear regression because it allows you to calculate the coefficients of the regression model by multiplying the inverse of the design matrix with the target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec4fcff-2109-422b-a0eb-5ed655f23716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Matrix Multiplication\n",
    "def matmul(A, B):\n",
    "    result = []\n",
    "    for i in range(len(A)):\n",
    "        row = []\n",
    "        for j in range(len(B[0])):\n",
    "            val = sum(A[i][k] * B[k][j] for k in range(len(B)))\n",
    "            row.append(val)\n",
    "        result.append(row)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac6552-3ce6-4ec8-8226-3ecacc1ca65e",
   "metadata": {},
   "source": [
    "With these mathematical basics, the **multiple linear regression** can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f97c72-588c-4468-b155-b2c4882b98d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    " # Multiple Linear Regression\n",
    "def multivariate_regression(X_raw, y):\n",
    "    X = [[1] + row for row in X_raw]\n",
    "    y_vec = [[val] for val in y]\n",
    "    \n",
    "    XT = transpose(X)\n",
    "    XTX = matmul(XT, X)\n",
    "    XTX_inv = invert_matrix(XTX)\n",
    "    XTy = matmul(XT, y_vec)\n",
    "    \n",
    "    beta = matmul(XTX_inv, XTy)\n",
    "    return [b[0] for b in beta]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a957264e-079d-4bd3-8e81-7c236c52b290",
   "metadata": {},
   "source": [
    "A slightly modified **predict function** is required to determine the respective y values for the underlying x values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1893ed4-24b0-4dda-9350-74997464512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Function\n",
    "def predict_multi(X_raw, beta):\n",
    "    X = [[1] + row for row in X_raw]\n",
    "    return [sum(b * x for b, x in zip(beta, row)) for row in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32746a-8c22-4f9c-bc17-a860a25d7ae1",
   "metadata": {},
   "source": [
    "Again, **residuals** represent the differences between the observed values and the predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0435ccc-7193-4e08-bc93-4e7f6e6b05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals\n",
    "def residuals_multi(X_raw, y, beta):\n",
    "    y_pred = predict_multi(X_raw, beta)\n",
    "    return [yi - y_hat for yi, y_hat in zip(y, y_pred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce7130-c5c8-4b85-8bc0-6a9b70171653",
   "metadata": {},
   "source": [
    "The **coefficient of determination** for a multiple linear regression model measures how well the model's predictions match the actual data. It indicates the proportion of the variance in the target variable that can be explained by the model.  It's interpretation is therefore similar to the coefficient of determination of a single linear regression model and may vary between 0 and 1, while a value closer to 0 means the model doesn't explain much of the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f839e-cf8a-4d4e-ab14-fdea0e1d6424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient of Determination\n",
    "def r_squared_multi(X_raw, y, beta):\n",
    "    y_pred = predict_multi(X_raw, beta)\n",
    "    y_mean = sum(y) / len(y)\n",
    "    \n",
    "    ss_tot = sum((yi - y_mean) ** 2 for yi in y)\n",
    "    ss_res = sum((yi - y_hat) ** 2 for yi, y_hat in zip(y, y_pred))\n",
    "    \n",
    "    return 1 - ss_res / ss_tot if ss_tot != 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f4eb2b-b2b2-4051-ab5a-838a44ddfbc1",
   "metadata": {},
   "source": [
    "Finally, these are **application examples** for the multiple linear regression coefficients with coresponding predictions for one case, the residuals of the model as well as the coefficient of determination in MicroPython:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9f5866-1df6-4520-857d-02772e98468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application Example\n",
    "beta = multivariate_regression(X, y)\n",
    "print(\"Coefficients:\")\n",
    "for i, b in enumerate(beta):\n",
    "    if i == 0:\n",
    "        print(\"a =\", b)\n",
    "    else:\n",
    "        print(\"b{} = {}\".format(i, b))\n",
    "\n",
    "x_case_13 = [X[12]]\n",
    "y_pred_13 = predict_multi(x_case_13, beta)[0]\n",
    "print(\"\\nPredictions for x1 = 11.4 and x2 = 76:\", y_pred_13)\n",
    "\n",
    "residuals = residuals_multi(X, y, beta)\n",
    "print(\"\\nResiduals:\", residuals)\n",
    "\n",
    "r2 = r_squared_multi(X, y, beta)\n",
    "print(\"\\nCoefficient of Determination:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5c822-e74d-4203-9702-c5c3ad840d2c",
   "metadata": {},
   "source": [
    "## Machine Learning: Classification\n",
    "### Multiple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45842089-f9ce-4c8c-8fdd-9277b4fbad40",
   "metadata": {},
   "source": [
    "Three variables of the **trees dataset**, provided by Atkinson, A. C. (1985): *Plots, Transformations and Regression* via Oxford University Press. The dependant variable has been dichotomized, whereby a volume greater than 20 results in 1, else 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2be138-cc96-4ed2-a667-fea18753c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girth (x1), Height (x2) and Binary Volume (y) of Black Cherry Trees\n",
    "X = [[8.3, 70], [8.6, 65], [8.8, 63], [10.5, 72], [10.7, 81], [10.8, 83], [11, 66], [11, 75], [11.1, 80], [11.2, 75],\n",
    "    [11.3, 79], [11.4, 76], [11.4, 76], [11.7, 69], [12, 75], [12.9, 74], [12.9, 85], [13.3, 86], [13.7, 71], [13.8, 64],\n",
    "    [14, 78], [14.2, 80], [14.5, 74], [16, 72], [16.3, 77], [17.3, 81], [17.5, 82], [17.9, 80], [18, 80], [18, 80], [20.6, 87]]\n",
    "\n",
    "\n",
    "y = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bcf888-fdd8-49ba-a48c-ac9e5c5ea9e1",
   "metadata": {},
   "source": [
    "The **sigmoid function** in (multiple) logistic regression maps any input value to a range between 0 and 1, allowing us to interpret the result as a probability by producing an S-shaped curve ideal for binary classification with 0=no and 1=yes, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31090e0b-f743-431c-901e-a94a43c353c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Sigmoid Function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + pow(2.71828, -z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9f9d4-d196-4158-ab04-d962feea5ec8",
   "metadata": {},
   "source": [
    "The **log function** approximates the natural logarithm using a numerical method based on the limit definition, useful when built-in log functions are unavailable in MicroPython. The natural logarithm (ln) is the inverse of the exponential function and tells us how many times we must multiply e≈2.71828 to get a given number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40867a5-b5fb-4806-821b-ab14ba743f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Log Function\n",
    "def log(x):\n",
    "    n = 1000.0\n",
    "    return n * ((x**(1/n)) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232ca42f-8e76-4c8a-9ecd-4b8a8015947d",
   "metadata": {},
   "source": [
    "As a result of these mathematical basics, a function for the **prediction of probabilities** is required for processing the values of the previous sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5504c-4bff-4a0e-b934-28f7c65a1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Prediction of Probability\n",
    "def predict_proba(x_input, weights, bias):\n",
    "    z = bias\n",
    "    for i in range(len(x_input)):\n",
    "        z += weights[i] * x_input[i]\n",
    "    return sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbeab7e-cd55-4337-9ade-89d56ea02a7b",
   "metadata": {},
   "source": [
    "This function trains a logistic regression model using **gradient descent**. It iteratively updates the weights and bias to minimize the error between predicted probabilities (from the sigmoid function) and actual labels. By adjusting the weights in the direction that reduces the loss, the model gradually learns to classify input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba09a35-fb83-4789-adf9-b81642424a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate Logistic Regression via Gradient Descent\n",
    "def train_logistic_regression(X, y, lr=0.01, epochs=5000):\n",
    "    n_samples = len(X)\n",
    "    n_features = len(X[0])\n",
    "    weights = [0] * n_features\n",
    "    bias = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        grad_w = [0] * n_features\n",
    "        grad_b = 0\n",
    "        for i in range(n_samples):\n",
    "            z = bias\n",
    "            for j in range(n_features):\n",
    "                z += weights[j] * X[i][j]\n",
    "            p = sigmoid(z)\n",
    "            error = p - y[i]\n",
    "            for j in range(n_features):\n",
    "                grad_w[j] += error * X[i][j]\n",
    "            grad_b += error\n",
    "        for j in range(n_features):\n",
    "            weights[j] -= lr * grad_w[j] / n_samples\n",
    "        bias -= lr * grad_b / n_samples\n",
    "\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fe6cc5-0222-4d7d-8755-091b181239f3",
   "metadata": {},
   "source": [
    "Again, a slightly modified **predict function** is required to determine the respective y values for the underlying x values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d0f35-1e82-4734-8e8d-b96820b46ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Prediction of Multivariate Logistic Regression\n",
    "def predict(x_input, weights, bias):\n",
    "    p = predict_proba(x_input, weights, bias)\n",
    "    return 1 if p >= 0.5 else 0, p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ccb60-abd3-47d3-8718-78b8c2d9e551",
   "metadata": {},
   "source": [
    "The **application examples** for the multiple logistic regression focus on weights and bias of the model and return logits and probabilities as values for classification. A classification example highlights the functionalty of multiple logistic regression models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c5c8d-6684-4a26-aa0e-1d092a2a9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application Examples\n",
    "weights, bias = train_logistic_regression(X, y)\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Intercept:\", bias)\n",
    "\n",
    "print(\"\\nLogits and Probabilities:\")\n",
    "for i in range(len(X)):\n",
    "    z = bias + sum([weights[j] * X[i][j] for j in range(len(weights))])\n",
    "    p = sigmoid(z)\n",
    "    print(\"x =\", X[i], \"Logit =\", z, \"P(y=1) =\", p)\n",
    "\n",
    "classification = predict([11.4, 76], weights, bias)\n",
    "print(\"\\nPredicted Class:\", classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be745d1-84c8-4610-916f-852a536aa5dc",
   "metadata": {},
   "source": [
    "## Machine Learning: Clustering\n",
    "### K-Means based upon Within Cluster Sum of Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddbf068-dd7c-4955-b762-04bde0d71420",
   "metadata": {},
   "source": [
    "Two variables and 15 cases of the original **trees dataset**, provided by Atkinson, A. C. (1985): *Plots, Transformations and Regression* via Oxford University Press. The other 15 cases are simulated trees, based upon another type of tree. Therefore, the dependant variable is dichotomized, indicating black cherry trees from the original dataset by 0 and simulated trees by 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377f9c9-5fc0-47a6-a315-3aba5f69c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girth (x1), Height (x2) and Class (y) of Black Cherry Trees and Simulated Trees\n",
    "X = [\n",
    "    [8.3, 70], [8.6, 65], [8.8, 63], [10.5, 72], [10.7, 81], [10.8, 83], [11.0, 66], [11.0, 75], [11.1, 80],\n",
    "    [11.2, 75], [11.3, 79], [11.4, 76], [11.7, 69], [12.0, 75], [12.9, 74], [5.2, 45], [5.5, 48], [6.0, 50],\n",
    "    [6.3, 46], [6.7, 49], [7.0, 51], [7.2, 47], [7.4, 52], [7.5, 50], [7.7, 46], [7.9, 53], [8.1, 49],\n",
    "    [8.4, 47], [8.5, 54], [8.7, 52]\n",
    "]\n",
    "\n",
    "# y = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1 ,1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025152de-6ebb-4bd0-b2a4-9d2c2c74bd0e",
   "metadata": {},
   "source": [
    "The **euclidean distance** measures the straight-line distance between two points in a multi-dimensional space, calculated as the square root of the sum of the squared differences between corresponding coordinates. It’s commonly used in clustering and classification tasks to determine similarity between data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0e8ae-3f0f-4e18-8173-83915a0c5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Basics - Euclidean Distance\n",
    "def euclidean_distance(p1, p2):\n",
    "    return sum((p1[i] - p2[i])**2 for i in range(len(p1))) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095a1a3-c5f8-4815-974a-71f4d13246c4",
   "metadata": {},
   "source": [
    "The **initializing centroids function** sets the starting points for the cluster centers and influences the convergence of the algorithm and the quality of the final clusters, as it determines how the data is grouped during the iterative process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1910808-8f03-420f-9570-1a110796da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Centroids Function\n",
    "def initialize_centroids(X, k):\n",
    "    return [X[i][:] for i in range(k)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068c2b9d-3088-4b0a-817b-337ac249fa53",
   "metadata": {},
   "source": [
    "The **assigning clusters function** groups data points into clusters based on their proximity to the centroids. For each point, it calculates the euclidean distance to each centroid and assigns the point to the closest centroid’s cluster, ensuring that each cluster contains the points nearest to its respective centroid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeaa203-e72b-4ed2-a077-d2725c17a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning Clusters Function\n",
    "def assign_clusters(X, centroids):\n",
    "    clusters = [[] for _ in centroids]\n",
    "    for point in X:\n",
    "        distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
    "        min_index = distances.index(min(distances))\n",
    "        clusters[min_index].append(point)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65757e-9a40-46e2-ba82-8aef7302cff9",
   "metadata": {},
   "source": [
    "The **computing centroids function** calculates the new centroids by finding the mean of all points within each cluster. For each cluster, it averages the values of each feature across all points, updating the centroid to represent the center of that cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d36a37-1443-47d6-84b4-3524e5e114a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Centroids Function\n",
    "def compute_centroids(clusters):\n",
    "    new_centroids = []\n",
    "    for cluster in clusters:\n",
    "        if not cluster:\n",
    "            continue\n",
    "        n_features = len(cluster[0])\n",
    "        mean = [0] * n_features\n",
    "        for point in cluster:\n",
    "            for i in range(n_features):\n",
    "                mean[i] += point[i]\n",
    "        mean = [val / len(cluster) for val in mean]\n",
    "        new_centroids.append(mean)\n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ea304-224d-4673-a8fb-c1757cee4087",
   "metadata": {},
   "source": [
    "The **within cluster sum of squares** is defined as the total squared distance between each point and its assigned cluster centroid. It measures the compactness of the clusters, with smaller values indicating tighter clusters. The code computes this by summing the squared differences for all points in each cluster, relative to the centroid of that cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2045070-1148-4e3d-a83d-c9b766bc815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within Cluster Sum of Squares\n",
    "def wcss(clusters, centroids):\n",
    "    total = 0\n",
    "    for i in range(len(clusters)):\n",
    "        for point in clusters[i]:\n",
    "            total += sum((point[j] - centroids[i][j])**2 for j in range(len(point)))\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f52ee-4c50-4e47-9908-17f55b1ce15e",
   "metadata": {},
   "source": [
    "The **k-means algorithm** groups data points into k clusters. It iteratively assigns points to the closest centroids, recalculates the centroids, and computes the within cluster sum of squares until the centroids no longer change or the maximum number of iterations is reached, returning the final within cluster sum of squares value to assess the clustering quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bf01b7-8537-48a2-a027-8fc91ef7199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Algorithm\n",
    "def kmeans_wcss(X, k=2, max_iter=100):\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    for _ in range(max_iter):\n",
    "        clusters = assign_clusters(X, centroids)\n",
    "        new_centroids = compute_centroids(clusters)\n",
    "        if new_centroids == centroids:\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "\n",
    "    labels = [0] * len(X)\n",
    "    for cluster_index, cluster_points in enumerate(clusters):\n",
    "        for point in cluster_points:\n",
    "            for idx, original_point in enumerate(X):\n",
    "                if original_point == point:\n",
    "                    labels[idx] = cluster_index\n",
    "                    break\n",
    "\n",
    "    return wcss(clusters, centroids), labels, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12459e55-0c51-4d2d-abb6-cb7e9b781f29",
   "metadata": {},
   "source": [
    "The **k-means indicator** highlights the chance of the within cluster sum of squares values when the number of centroids is increased. A decreasing value indicates a better allocation of the cases to the centroids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab3927-cf15-4914-b899-43470bf6f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Indicator\n",
    "def kmeans_indicator(X, max_k=10):\n",
    "    wcss_values = []\n",
    "    for k in range(1, max_k + 1):\n",
    "        wcss_value, _, _ = kmeans_wcss(X, k)\n",
    "        wcss_values.append(wcss_value)\n",
    "    return wcss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce3bff1-870c-47a5-9494-4fa1aec3201a",
   "metadata": {},
   "source": [
    "The **application examples** indicate the within cluster sum of squares for each number of clusters. In addition, it indicates the number of clusters within the dataset, which in this case is supposed to be 2 and assigns the labels accordingly. The position of the centroids is highlighted as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbbc3bb-c6d0-4ce3-83ae-eaca0fd849b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Application examples\n",
    "wcss_list = kmeans_indicator(X, max_k=6)\n",
    "\n",
    "print(\"WCSS values for k = 1 to 6:\")\n",
    "for k, w in enumerate(wcss_list, 1):\n",
    "    print(\"k =\", k, \"-> WCSS =\", w)\n",
    "\n",
    "wcss_value, cluster_labels, final_centroids = kmeans_wcss(X, k=2)\n",
    "\n",
    "print(\"\\nWCSS:\", wcss_value)\n",
    "print(\"\\nCluster Labels:\", cluster_labels)\n",
    "print(\"\\nCentroids:\", final_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d27927-2434-4ca6-9bec-93ff6597d2ef",
   "metadata": {},
   "source": [
    "# Machine Learning: Dimensionality Reduction\n",
    "## Exploratory Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc57607-4fb3-477d-95d4-1e02bada8c93",
   "metadata": {},
   "source": [
    "These are 10 variables, based upon 5 variables each for the two personality dimensions extraversion and neuroticism, from the **bfi data** by Revelle, Wilt and Rosenthal (2010): *Individual Differences in Cognition: New Methods for examining the Personality-Cognition Link* via Springer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbddabab-f2b7-4f22-af4b-d9d4381d587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraversion (x1:x5) and Neuroticism (x6:x10) of the Big Five Inventory\n",
    "X = [\n",
    "    [2, 1, 6, 5, 6, 3, 5, 2, 2, 3],\n",
    "    [3, 6, 4, 2, 1, 6, 3, 2, 6, 4],\n",
    "    [1, 3, 2, 5, 4, 3, 3, 4, 2, 3],\n",
    "    [3, 4, 3, 6, 5, 2, 4, 2, 2, 3],\n",
    "    [2, 1, 2, 5, 2, 2, 2, 2, 2, 2],\n",
    "    [2, 2, 4, 6, 6, 4, 4, 4, 6, 6],\n",
    "    [3, 2, 5, 5, 6, 2, 3, 3, 1, 1],\n",
    "    [1, 1, 6, 6, 6, 2, 3, 1, 2, 1],\n",
    "    [2, 4, 4, 2, 6, 3, 3, 5, 3, 2],\n",
    "    [1, 2, 6, 5, 4, 1, 4, 2, 2, 5],\n",
    "    [1, 2, 6, 5, 5, 5, 4, 4, 3, 1],\n",
    "    [1, 2, 4, 5, 5, 3, 2, 4, 1, 2],\n",
    "    [6, 6, 2, 1, 1, 1, 2, 1, 3, 6],\n",
    "    [3, 4, 3, 2, 3, 5, 3, 4, 4, 3],\n",
    "    [6, 6, 3, 2, 2, 2, 2, 2, 4, 1],\n",
    "    [3, 4, 3, 3, 5, 5, 6, 5, 5, 4],\n",
    "    [3, 2, 3, 6, 5, 1, 2, 1, 2, 1],\n",
    "    [4, 3, 4, 4, 4, 2, 2, 3, 3, 3],\n",
    "    [3, 3, 2, 5, 4, 2, 3, 1, 3, 2],\n",
    "    [6, 4, 4, 4, 3, 2, 2, 3, 4, 5]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dae611-11a7-4742-9640-a930bf58ac94",
   "metadata": {},
   "source": [
    "Mean centering via a **mean center function**, is often used in data preprocessing to make the dataset more suitable for machine learning algorithms by ensuring all features contribute equally to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b721b47-c843-4ef7-a9ee-ab7a43822adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Center Function\n",
    "def mean_center(X):\n",
    "    cols = len(X[0])\n",
    "    rows = len(X)\n",
    "    means = [sum(X[i][j] for i in range(rows)) / rows for j in range(cols)]\n",
    "    centered = [[X[i][j] - means[j] for j in range(cols)] for i in range(rows)]\n",
    "    return centered, means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe74ba5-5d78-408e-9114-0ae46358a662",
   "metadata": {},
   "source": [
    "Again, the **correlation matrix** quantifies the strength and direction of the linear relationship between two variables. This code can be used to correlate several variables and summarize the corresponding values in one matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79a7480-ba0a-4a0a-baed-c9b8336cbd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "def correlation_matrix(X):\n",
    "    rows = len(X)\n",
    "    cols = len(X[0])\n",
    "    corr = [[0]*cols for _ in range(cols)]\n",
    "\n",
    "    for i in range(cols):\n",
    "        for j in range(cols):\n",
    "            xi = [row[i] for row in X]\n",
    "            xj = [row[j] for row in X]\n",
    "            num = sum(xi[k] * xj[k] for k in range(rows))\n",
    "            denom_i = sum(xi[k]**2 for k in range(rows)) ** 0.5\n",
    "            denom_j = sum(xj[k]**2 for k in range(rows)) ** 0.5\n",
    "            corr[i][j] = num / (denom_i * denom_j)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f312c679-c3a6-4080-8a2e-1f886fbf3130",
   "metadata": {},
   "source": [
    "The **power iteration function** is an algorithm used to compute the dominant eigenvalues and eigenvectors of a matrix. The process involves iteratively applying matrix-vector multiplication to a random initial vector and normalizing it to avoid overflow or underflow, which allows the vector to converge to the eigenvector corresponding to the largest eigenvalue: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22cf945-5b74-43db-8441-cb8f02485093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power Iteration Function \n",
    "def power_iteration(A, num_vectors=2, iterations=100):\n",
    "    n = len(A)\n",
    "    eigenvectors = []\n",
    "    eigenvalues = []\n",
    "\n",
    "    for _ in range(num_vectors):\n",
    "        b = [1.0]*n\n",
    "        for _ in range(iterations):\n",
    "            # Multiply A * b\n",
    "            Ab = [sum(A[i][j] * b[j] for j in range(n)) for i in range(n)]\n",
    "            norm = sum(x**2 for x in Ab) ** 0.5\n",
    "            b = [x / norm for x in Ab]\n",
    "        # Rayleigh quotient for eigenvalue\n",
    "        Ab = [sum(A[i][j] * b[j] for j in range(n)) for i in range(n)]\n",
    "        eigval = sum(b[i] * Ab[i] for i in range(n))\n",
    "        eigenvalues.append(eigval)\n",
    "        eigenvectors.append(b)\n",
    "\n",
    "        # Deflation\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                A[i][j] -= eigval * b[i] * b[j]\n",
    "\n",
    "    return eigenvalues, eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f8a0e2-d94d-4e0e-bf19-03d4bb530b33",
   "metadata": {},
   "source": [
    "The **factor loadings function** computes the factor loadings based on the correlation matrix, eigenvalues, and eigenvectors. In factor analysis, factor loadings represent the relationships between observed variables and the underlying latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521b949-7c15-43eb-a159-fec7fce4fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor Loadings Function\n",
    "def factor_loadings(corr_matrix, eigenvalues, eigenvectors):\n",
    "    loadings = []\n",
    "    for i in range(len(corr_matrix)):\n",
    "        row = []\n",
    "        for j in range(len(eigenvectors)):\n",
    "            loading = eigenvectors[j][i] * (eigenvalues[j] ** 0.5)\n",
    "            row.append(loading)\n",
    "        loadings.append(row)\n",
    "    return loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bbb18a-86da-494e-86e1-5d9288c44459",
   "metadata": {},
   "source": [
    "This **application example** shows how to compute the correlation matrix, the eigenvalues as well as the corresponding factor loadings for identifying the underlying factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e727248-f8be-4e51-b488-ab9bea3ac908",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered, means = mean_center(X)\n",
    "R = correlation_matrix(X_centered)\n",
    "eigvals, eigvecs = power_iteration([row[:] for row in R], num_vectors=2)\n",
    "loadings = factor_loadings(R, eigvals, eigvecs)\n",
    "\n",
    "print(\"Correlation matrix:\")\n",
    "for row in R:\n",
    "    print([\"{0:.2f}\".format(x) for x in row])\n",
    "\n",
    "print(\"\\nEigenvalues:\")\n",
    "for i, val in enumerate(eigvals):\n",
    "    print(\"Factor\", i+1, \":\", round(val, 3))\n",
    "\n",
    "print(\"\\nFactor Loadings:\")\n",
    "for i, row in enumerate(loadings):\n",
    "    print(\"V\" + str(i+1), \":\", [\"{0:.2f}\".format(x) for x in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f5d63-0721-4f5b-85dd-156d5fe9e6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
